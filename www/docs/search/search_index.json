{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Demand Side Platform \u00b6 A complete, open source advertising Demand Side Platform (DSP). Completely \"Dockerized\" and uses well known open source tools. Contains 3 major components: OpenRTB Real Time Bidder: Written in Java 1.11 and implements RTB v 2.8. Campaign Manager: React based campaign manager for controlling campaigns. Uses Postgres for database operations. Data Management Platform: A DMP developed with Elastic Search, Logstash, and Kibana. OpenRTB Engine 2.8 \u00b6 OPENRTB Bidding engine (2.8) in Java 1.8. Supports banner, native, audio and video ads. Try the stand-alone bidder in your environment using Docker-Compose in your environment: Campaign Manager \u00b6 Try the React-based stand-alone campaign manager in your environment using Docker-Compose: Data Management Platform \u00b6 Data Management Platform (DMP) provides campaign and system analytics with the ELK Stack, Try the stand-alone DMP in your environment using Docker-Compose: Kubernetes Deployments \u00b6 Deploy your DSP in your environment, or in Amazon Web Services (AWS). E-mail: info@rtb4free.com","title":"Home"},{"location":"#demand-side-platform","text":"A complete, open source advertising Demand Side Platform (DSP). Completely \"Dockerized\" and uses well known open source tools. Contains 3 major components: OpenRTB Real Time Bidder: Written in Java 1.11 and implements RTB v 2.8. Campaign Manager: React based campaign manager for controlling campaigns. Uses Postgres for database operations. Data Management Platform: A DMP developed with Elastic Search, Logstash, and Kibana.","title":"Demand Side Platform"},{"location":"#openrtb-engine-28","text":"OPENRTB Bidding engine (2.8) in Java 1.8. Supports banner, native, audio and video ads. Try the stand-alone bidder in your environment using Docker-Compose in your environment:","title":"OpenRTB Engine 2.8"},{"location":"#campaign-manager","text":"Try the React-based stand-alone campaign manager in your environment using Docker-Compose:","title":"Campaign Manager"},{"location":"#data-management-platform","text":"Data Management Platform (DMP) provides campaign and system analytics with the ELK Stack, Try the stand-alone DMP in your environment using Docker-Compose:","title":"Data Management Platform"},{"location":"#kubernetes-deployments","text":"Deploy your DSP in your environment, or in Amazon Web Services (AWS). E-mail: info@rtb4free.com","title":"Kubernetes Deployments"},{"location":"home/building/","text":"Building the System \u00b6 The source code for the RTB4FREE system is located here Makefile \u00b6 To work with the source you will need Maven, Java 1.11, Make, and Yarn, and Mkdocs. The goal of the makefile is to create the docker container necessary for deploying the bidder. First step is to git pull the distribution. Application \u00b6 The 'make application', or simply 'make' will create all the components necessary for the docker container. $make application This will compile the JAVA application into an all-in-one JAR file, it will then create the campaign manager react application, and then create the docker container. Local \u00b6 The 'make local' compiles just the JAVA application. Eg: $make local This just creates the classes, it does not build the JAR file. React \u00b6 The 'make react' command compiles the REACT based campaign manager into a deployable www directory. $make react This uses YARN to make the system. Docker \u00b6 The 'make docker' command creates the docker container that holds the react-based campaign manager and the JAVA based bidder. It does not compile these artifacts first. $make docker This will create the linux image used by the applications, but does not compile them. Backup-db \u00b6 This command will back up your current POSTGRES database into a dump file. $make backup-db By default it connects to localhost:54321, you will need to adjust this to point to the postgres container if localhost is not the destination. It creates a file called 'database.backup' in the current working directory. Restore-db \u00b6 This command will restore the database.backup file to the postgres database. Note, By default it connects to localhost:54321, you will need to adjust this to point to the postgres container if localhost is not the destination. $make restore-db Clean \u00b6 This command deletes the class files. $make clean","title":"Building"},{"location":"home/building/#building-the-system","text":"The source code for the RTB4FREE system is located here","title":"Building the System"},{"location":"home/building/#makefile","text":"To work with the source you will need Maven, Java 1.11, Make, and Yarn, and Mkdocs. The goal of the makefile is to create the docker container necessary for deploying the bidder. First step is to git pull the distribution.","title":"Makefile"},{"location":"home/building/#application","text":"The 'make application', or simply 'make' will create all the components necessary for the docker container. $make application This will compile the JAVA application into an all-in-one JAR file, it will then create the campaign manager react application, and then create the docker container.","title":"Application"},{"location":"home/building/#local","text":"The 'make local' compiles just the JAVA application. Eg: $make local This just creates the classes, it does not build the JAR file.","title":"Local"},{"location":"home/building/#react","text":"The 'make react' command compiles the REACT based campaign manager into a deployable www directory. $make react This uses YARN to make the system.","title":"React"},{"location":"home/building/#docker","text":"The 'make docker' command creates the docker container that holds the react-based campaign manager and the JAVA based bidder. It does not compile these artifacts first. $make docker This will create the linux image used by the applications, but does not compile them.","title":"Docker"},{"location":"home/building/#backup-db","text":"This command will back up your current POSTGRES database into a dump file. $make backup-db By default it connects to localhost:54321, you will need to adjust this to point to the postgres container if localhost is not the destination. It creates a file called 'database.backup' in the current working directory.","title":"Backup-db"},{"location":"home/building/#restore-db","text":"This command will restore the database.backup file to the postgres database. Note, By default it connects to localhost:54321, you will need to adjust this to point to the postgres container if localhost is not the destination. $make restore-db","title":"Restore-db"},{"location":"home/building/#clean","text":"This command deletes the class files. $make clean","title":"Clean"},{"location":"home/deploying/","text":"Deployments \u00b6 Docker-Compose \u00b6 Kubernetes \u00b6 This simple quickstart deployment will start the RTB4FREE stack on a single k8s node. It will start the following containers: One RTB4FREE bidder. One postgresql database server. Contains campaign configuration information used by the bidder. One zookeeper instance. Used by kafka. One kafka instance. Data from the bidder is published to Kafka, and is read by logstash. One Elasticsearch node. Contains bidder logs on campaign activity such as requests, bids, wins, etc. One Kibana node. User interface application for Elasticsearch. One Logstash instance. Reads data published to Kafka from the bidder, transforms data, then indexes into Elasticsearch. This configuration was developed on a multipass virtual machine and a k3s kubernetes single node cluster. The setup instructions for this configuration is described here Start the quickstart deployment with the following kubectl commands. Start Postgres. kubectl apply -f postgres-pod.yml Start Kafka and Zookeeper. kubectl apply -f kafka-pod.yml Get ECK from Elastic kubectl apply -f https://download.elastic.co/downloads/eck/1.1.2/all-in-one.yaml Start Elasticsearch and Kibana kubectl apply -f quickstart/eck_quickstart.yml kubectl apply -f quickstart/eck_kibana_quickstart.yml Get user \"elastic\", pwd for kibana login kubectl get secret quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo Port forward kibana port kubectl port-forward service/quickstart-kb-http 5601 & Log in to Kibana and confirm Elasticsearch is up and active. Open a browser to https://localhost:5601 Note: https! You may need to allow your browser to access https from localhost Log in with user \"elastic\" and password from previous step. Start Logstash. kubectl apply -f quickstart/logstash_quickstart.yml At this point, Logstash should have connected to Elasticsearch. Logstash will load index templates used by the bidder. Logstash should now also be connected to Kafka and listening for bidder topics. Start the bidder and open the ports used. kubectl apply -f quickstart/rtb_quickstart.yml kubectl port-forward service/rtb-bidder-service 8155 & kubectl port-forward service/rtb-bidder-service 7379 & kubectl port-forward service/rtb-bidder-service 8080 & Connectivity from the bidder to Kafka to Logstash to Elasticsearch should now be established. . Go to the Kibana, Dev Tools. List indices with command: GET _cat/indices?s=i You should now see indices for rtblogs- and status- . You can define campaigns in the bidder's Campaign Manger. Open a browser to http://localhost:8155. Select Campaign Manager and log in to defined campaigns. Load the Elasticsearch Data Transform jobs that calculate campaign summaries and budgets. Go to the Kibana, Dev Tools To load data transform jobs for campaign summaries, copy these API commands to the development console Execute. To load data transform jobs for budget summaries, copy these API commands to the development console Execute. Go to the Kibana, Stack Management, Elasticsearch - Transforms. You should see the 6 transform jobs. Start all the Transform jobs. Load the Kibana objects. Go to the Kibana, Stack Management, Kibana - Saved Objects. Choose Import, and load the saved objects file for bidder . This loads the index patterns, visualizations and dashboards The RTB4FREE stack should be up and running, and able to process bid requests. Requests, bids, pixels and wins are logged into Elasticsearch. Results will be viewable on the Campaign Manager Dashboard and in the Kibana Dashboards. Start Demo Campaigns \u00b6 Load the demo postgres database. Copy to demo postgres database into your postgres pod. kubectl cp ./postgres_db/rtb_postgres.dump postgres:rtb_postgres.dump Import the demo configuration. kubectl exec -it postgres /bin/bash psql -U postgres postgres < rtb_postgres.dump exit View/Edit the campaign in bidder's Campaign Manger. Open a browser to http://localhost:8155 Log in with credentials Org: rtb4free User: ben.faul@rtb4free.com Password: zulu Server: localhost:7379 You can view the demo Campaign, Creative and Target definitions. Selecting Campaigns should show the active campaign in running mode. Generate sample requests to show bidder operation. Start simulator with command: kubectl apply -f simulator.yml After the bidder starts processing bids, you should see results in the Campaign Manger and Elasticsearch. Select the Dashboard. You should see active campaign with stats. In Kibana, the RTB Dashboard should show activity. On Elasticsearch, the following indices with campaign data will be generated. budget_daily budget_hourly budget_total pixels-yyyy.mm.dd requests-yyyy.mm.dd rtblogs-yyyy.mm.dd status-yyyy.mm.dd wins-<yyyy.mm.dd xform-bids xform-pixels xform-wins Cluster deployment \u00b6 The quickstart deployment will start the RTB4FREE stack on a single k8s node. It will start the following containers: Three RTB4FREE bidders. One postgresql database server. One zookeeper instance. One kafka instance. Three node Elasticsearch cluster. Two Kibana nodes. Three Logstash instances. This configuration was developed on 3 multipass virtual machines as a 3 node k3s kubernetes cluster. The setup instructions for this configuration is described here Start the cluster deployment with the following kubectl commands. Start Postgres. kubectl apply -f postgres-pod.yml Start Kafka and Zookeeper. kubectl apply -f kafka-pod.yml Get ECK from Elastic kubectl apply -f https://download.elastic.co/downloads/eck/1.1.2/all-in-one.yaml Create local Persistent Volumes for Elasticsearch data on nodes 1, 2 and 3 multipass shell node1 sudo mkdir /esdata_eck exit Start Elasticsearch and Kibana kubectl apply -f cluster/eck_cluster.yml kubectl apply -f cluster/eck_kibana_cluster.yml Get user \"elastic\", pwd for kibana login kubectl get secret es-cluster-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo Port forward kibana port kubectl port-forward service/kb-cluster-kb-http 5601 & Log in to Kibana and confirm Elasticsearch is up and active. Open a browser to https://localhost:5601 Note: https! You may need to allow your browser to access https from localhost Log in with user \"elastic\" and password from previous step. Start Logstash. kubectl apply -f cluster/logstash_cluster.yml At this point, Logstash should have connected to Elasticsearch. Logstash will load index templates used by the bidder. Logstash should now also be connected to Kafka and listening for bidder topics. Start the bidder and open the ports used. kubectl apply -f cluster/rtb_cluster.yml kubectl port-forward service/rtb-bidder-service 8155 & kubectl port-forward service/rtb-bidder-service 7379 & kubectl port-forward service/rtb-bidder-service 8080 & Connectivity from the bidder to Kafka to Logstash to Elasticsearch should now be established. . Go to the Kibana, Dev Tools. List indices with command: GET _cat/indices?s=i You should now see indices for rtblogs- and status- . You can define campaigns in the bidder's Campaign Manger. Open a browser to http://localhost:8155. Select Campaign Manager and log in to defined campaigns. Load the Elasticsearch Data Transform jobs that calculate campaign summaries and budgets. Go to the Kibana, Dev Tools To load data transform jobs for campaign summaries, copy these API commands to the development console Execute. To load data transform jobs for budget summaries, copy these API commands to the development console Execute. Go to the Kibana, Stack Management, Elasticsearch - Transforms. You should see the 6 transform jobs. Start all the Transform jobs. Load the Kibana objects. Go to the Kibana, Stack Management, Kibana - Saved Objects. Choose Import, and load the saved objects file for bidder . This loads the index patterns, visualizations and dashboards The RTB4FREE stack should be up and running, and able to process bid requests. Requests, bids, pixels and wins are logged into Elasticsearch. Results will be viewable on the Campaign Manager Dashboard and in the Kibana Dashboards.","title":"Deploying"},{"location":"home/deploying/#deployments","text":"","title":"Deployments"},{"location":"home/deploying/#docker-compose","text":"","title":"Docker-Compose"},{"location":"home/deploying/#kubernetes","text":"This simple quickstart deployment will start the RTB4FREE stack on a single k8s node. It will start the following containers: One RTB4FREE bidder. One postgresql database server. Contains campaign configuration information used by the bidder. One zookeeper instance. Used by kafka. One kafka instance. Data from the bidder is published to Kafka, and is read by logstash. One Elasticsearch node. Contains bidder logs on campaign activity such as requests, bids, wins, etc. One Kibana node. User interface application for Elasticsearch. One Logstash instance. Reads data published to Kafka from the bidder, transforms data, then indexes into Elasticsearch. This configuration was developed on a multipass virtual machine and a k3s kubernetes single node cluster. The setup instructions for this configuration is described here Start the quickstart deployment with the following kubectl commands. Start Postgres. kubectl apply -f postgres-pod.yml Start Kafka and Zookeeper. kubectl apply -f kafka-pod.yml Get ECK from Elastic kubectl apply -f https://download.elastic.co/downloads/eck/1.1.2/all-in-one.yaml Start Elasticsearch and Kibana kubectl apply -f quickstart/eck_quickstart.yml kubectl apply -f quickstart/eck_kibana_quickstart.yml Get user \"elastic\", pwd for kibana login kubectl get secret quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo Port forward kibana port kubectl port-forward service/quickstart-kb-http 5601 & Log in to Kibana and confirm Elasticsearch is up and active. Open a browser to https://localhost:5601 Note: https! You may need to allow your browser to access https from localhost Log in with user \"elastic\" and password from previous step. Start Logstash. kubectl apply -f quickstart/logstash_quickstart.yml At this point, Logstash should have connected to Elasticsearch. Logstash will load index templates used by the bidder. Logstash should now also be connected to Kafka and listening for bidder topics. Start the bidder and open the ports used. kubectl apply -f quickstart/rtb_quickstart.yml kubectl port-forward service/rtb-bidder-service 8155 & kubectl port-forward service/rtb-bidder-service 7379 & kubectl port-forward service/rtb-bidder-service 8080 & Connectivity from the bidder to Kafka to Logstash to Elasticsearch should now be established. . Go to the Kibana, Dev Tools. List indices with command: GET _cat/indices?s=i You should now see indices for rtblogs- and status- . You can define campaigns in the bidder's Campaign Manger. Open a browser to http://localhost:8155. Select Campaign Manager and log in to defined campaigns. Load the Elasticsearch Data Transform jobs that calculate campaign summaries and budgets. Go to the Kibana, Dev Tools To load data transform jobs for campaign summaries, copy these API commands to the development console Execute. To load data transform jobs for budget summaries, copy these API commands to the development console Execute. Go to the Kibana, Stack Management, Elasticsearch - Transforms. You should see the 6 transform jobs. Start all the Transform jobs. Load the Kibana objects. Go to the Kibana, Stack Management, Kibana - Saved Objects. Choose Import, and load the saved objects file for bidder . This loads the index patterns, visualizations and dashboards The RTB4FREE stack should be up and running, and able to process bid requests. Requests, bids, pixels and wins are logged into Elasticsearch. Results will be viewable on the Campaign Manager Dashboard and in the Kibana Dashboards.","title":"Kubernetes"},{"location":"home/deploying/#start-demo-campaigns","text":"Load the demo postgres database. Copy to demo postgres database into your postgres pod. kubectl cp ./postgres_db/rtb_postgres.dump postgres:rtb_postgres.dump Import the demo configuration. kubectl exec -it postgres /bin/bash psql -U postgres postgres < rtb_postgres.dump exit View/Edit the campaign in bidder's Campaign Manger. Open a browser to http://localhost:8155 Log in with credentials Org: rtb4free User: ben.faul@rtb4free.com Password: zulu Server: localhost:7379 You can view the demo Campaign, Creative and Target definitions. Selecting Campaigns should show the active campaign in running mode. Generate sample requests to show bidder operation. Start simulator with command: kubectl apply -f simulator.yml After the bidder starts processing bids, you should see results in the Campaign Manger and Elasticsearch. Select the Dashboard. You should see active campaign with stats. In Kibana, the RTB Dashboard should show activity. On Elasticsearch, the following indices with campaign data will be generated. budget_daily budget_hourly budget_total pixels-yyyy.mm.dd requests-yyyy.mm.dd rtblogs-yyyy.mm.dd status-yyyy.mm.dd wins-<yyyy.mm.dd xform-bids xform-pixels xform-wins","title":"Start Demo Campaigns"},{"location":"home/deploying/#cluster-deployment","text":"The quickstart deployment will start the RTB4FREE stack on a single k8s node. It will start the following containers: Three RTB4FREE bidders. One postgresql database server. One zookeeper instance. One kafka instance. Three node Elasticsearch cluster. Two Kibana nodes. Three Logstash instances. This configuration was developed on 3 multipass virtual machines as a 3 node k3s kubernetes cluster. The setup instructions for this configuration is described here Start the cluster deployment with the following kubectl commands. Start Postgres. kubectl apply -f postgres-pod.yml Start Kafka and Zookeeper. kubectl apply -f kafka-pod.yml Get ECK from Elastic kubectl apply -f https://download.elastic.co/downloads/eck/1.1.2/all-in-one.yaml Create local Persistent Volumes for Elasticsearch data on nodes 1, 2 and 3 multipass shell node1 sudo mkdir /esdata_eck exit Start Elasticsearch and Kibana kubectl apply -f cluster/eck_cluster.yml kubectl apply -f cluster/eck_kibana_cluster.yml Get user \"elastic\", pwd for kibana login kubectl get secret es-cluster-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo Port forward kibana port kubectl port-forward service/kb-cluster-kb-http 5601 & Log in to Kibana and confirm Elasticsearch is up and active. Open a browser to https://localhost:5601 Note: https! You may need to allow your browser to access https from localhost Log in with user \"elastic\" and password from previous step. Start Logstash. kubectl apply -f cluster/logstash_cluster.yml At this point, Logstash should have connected to Elasticsearch. Logstash will load index templates used by the bidder. Logstash should now also be connected to Kafka and listening for bidder topics. Start the bidder and open the ports used. kubectl apply -f cluster/rtb_cluster.yml kubectl port-forward service/rtb-bidder-service 8155 & kubectl port-forward service/rtb-bidder-service 7379 & kubectl port-forward service/rtb-bidder-service 8080 & Connectivity from the bidder to Kafka to Logstash to Elasticsearch should now be established. . Go to the Kibana, Dev Tools. List indices with command: GET _cat/indices?s=i You should now see indices for rtblogs- and status- . You can define campaigns in the bidder's Campaign Manger. Open a browser to http://localhost:8155. Select Campaign Manager and log in to defined campaigns. Load the Elasticsearch Data Transform jobs that calculate campaign summaries and budgets. Go to the Kibana, Dev Tools To load data transform jobs for campaign summaries, copy these API commands to the development console Execute. To load data transform jobs for budget summaries, copy these API commands to the development console Execute. Go to the Kibana, Stack Management, Elasticsearch - Transforms. You should see the 6 transform jobs. Start all the Transform jobs. Load the Kibana objects. Go to the Kibana, Stack Management, Kibana - Saved Objects. Choose Import, and load the saved objects file for bidder . This loads the index patterns, visualizations and dashboards The RTB4FREE stack should be up and running, and able to process bid requests. Requests, bids, pixels and wins are logged into Elasticsearch. Results will be viewable on the Campaign Manager Dashboard and in the Kibana Dashboards.","title":"Cluster deployment"},{"location":"home/exchanges/","text":"Gernerally, any Open RTB SSP will work with RTB4FREE once configured. The following have been integrated with RTB4FREE in the past: admedia adprudence adventurefeeds appnexus atomx axonix bidswitch cappture citenko epomx fyber google gotham index intango kadam\" medianexusnetwork nexage openssp openx pubmatic republer smaato smartadserver smartyads ssphwy stroer taggify vdopia vertamedia waardx wideorbit xapads","title":"Exchanges"},{"location":"home/overview/","text":"Overview \u00b6 Bidding Engine \u00b6 IMDG \u00b6 Many different technologies have been tried for maintaining state information between multiple bidders and the campaign manager: Redis and Aerospike being most notable. In the end tbese technologies brought more bloat than value. What we really need is a cache between the bidders, and the campaign manager that used familiar JAVA structures, like Maps. That's exactly what Hazelcast offers for us, with its \"In Memory Data Grid\" IMDG. All of the shared data needed by the bidders, is stored in the IMDG. This includes all bids, wins, campaigns, and performance statistics. Furthermore, the bidders back each other's part of the DB up. Hazelcast is a clustered, fault tolerant customer data platform, that utilizes the standard JAVA Maplike structures. Each bidder is a hazelcast member, and ses the raft consensus protocol to create a fault tolerant database spread across clusters of datagrids. The total size of the database is limited by how many nodes you have running * the memory they have on board. Here's how the system is configured: The overall data base is the sum of the clustered data. In addition, the nodes back each other up, up to a limit of 6 backup nodes for each cluster. Using the Raft consensus protocol, one Bidder is the leader, and handles all Postgres initialization as well as budgeting. Any bidder can handle Campaign manager requests, but in the end they are all handled by the Lead bidder. If that Lead bidder, goes down, the other bidders will elect a new Leader. Budgeting and campaign management activities will not be interrupted. A campaign manager connected to a non-leader would likely not even notice the change in leaders. The IMDG architecture is quite easy to work with. An overview of its features can be seen here: Check out Hazelcast for more information Campaign Manager \u00b6 The campaign manager of RTB4FREE is a REACT based application that will load from any bidder. The campaign manager communicates with the Postgres database to store campaign information. Loading campaigns from Postgres into the Hazelcast cache is done by the Leader bidder running a thread called \"Crosstalk\". Postgres Database \u00b6 A postgres database stores all campaign definitions. The REACT-based campaign manager manipulates the postgres based tables using a well defined SQL schema, through a simple REST API to any bidder in an RTB4FREE cluster. But in reality, you can write your own RTB4FREE campaign manager by defining the appropriate tables in the SQL database and issuing the simple REST based commands to reload campaigns. The schema for the database is located here Data Management Platform \u00b6 The Data Management Platform (DMP), the third layer of the DSP. Layer 1 is the Bidder, Layer 2 is the Campaign Manager and Layer 3 is the Data management Platform. Features \u00b6 Integrated with the open source Elasticsearch, Logstash, Kibana (ELK) stack. Search and view RTB transaction log details for requests, bids, wins, pixels, clicks and costs. Search and view RTB application log details for bidder. Out-of-the-box dashboard reports in Kibana. Standard campaign metrics viewable in the Campaign Manager. Create custom reports using Kibana. Open architecture allows integration with your own data analytics infrastructure. Standard data backup and maintenance processes included. Scale up horizontally by deploying clusters for ELK and Kafka. All bidder logs are sent to Kafka, as shown in the following diagram. The system configuration depends on many parameters, including Bidder transaction rate (ie, queries per second). Performance of infrastructure. Available disk space. Historical data retention required. Granularity of data retained. The RTB4FREE architecture can scale to support high data processing rates by horizontally scaling any of the following components: Bidder servers can be added as required to support additional SSP rates. Kafka, which is used for transport of data between bidders and data stores, can be clustered. Kafka consumers (such as Logstash) can be partitioned to share data transfer responsibilities. Elasticsearch can be clustered to accomodate increased data rates or storage requirements. A detailed system configuration can be determined after understanding the detailed on the requirements. Kibana Reports \u00b6 The RTB log records can be searched by selecting the corresponding index from the dropdown list, then entering a Lucene query in the seach input box. The following record types can be selected. Requests. Requests for bids received from SSP exchanges. Bids. Bid responses to requests. The bidder will respond if a request matches a campaign. Wins. Winning bids from the exchange. Pixels. Impressions that have been served, typically signaled from a pixel image. Clicks. Click events. Postback Events. Additional user defined events, such as video events, mobile application loads, etc. Reasons. If a campaign has not bid on a request, the reason is logged here. This helps tune campaign paramters to increase win rates. RTB Logs. Bidder application log, used to determine health status of bidders. Stats. Bidder usage metrics, used to monitor bidder performance. Kibana contains a report builder that let's you create various display widgets, such as charts, graphs, maps, heat maps, tag clouds, etc. A sample time series graph showing request rate over time is shown below: You can combine various visualizations into a dashboard. This dashboard shows bidder transaction activity as well as bidder performance metrics in a single view. Any data in the logs can be visualized with these tools. Campaign Manager Reports \u00b6 Data logged into Elasticsearch can be made available to users of the Campaign Manger. The following Campaign Manager view shows each campaign's usage using data extracted from Elasticsearch. Custom Data Stores \u00b6 If you have special processing needs, you can build you own app to consume and process the data. An easy method is to build your own Kafka consumer and subscribe to the data topics you wish to process. The open source secor project let's you read any of the RTB logs and store them in cloud storage, such as Amazon S3 or Google Cloud Storage. This option offers unlimited storage without worrying about disk space. You can then download the data for offline data analysis using tools like Hadoop. If you need to process analytics in real time, a custom Kafka consumer can read the log stream and perform real-time analysis and store in your own database. Deployments \u00b6 For development and testing you can use docker-compose. For production use Kubernetes. Docker Compose \u00b6 Kubernetes \u00b6 Kubernetes on AWS \u00b6","title":"Overview"},{"location":"home/overview/#overview","text":"","title":"Overview"},{"location":"home/overview/#bidding-engine","text":"","title":"Bidding Engine"},{"location":"home/overview/#imdg","text":"Many different technologies have been tried for maintaining state information between multiple bidders and the campaign manager: Redis and Aerospike being most notable. In the end tbese technologies brought more bloat than value. What we really need is a cache between the bidders, and the campaign manager that used familiar JAVA structures, like Maps. That's exactly what Hazelcast offers for us, with its \"In Memory Data Grid\" IMDG. All of the shared data needed by the bidders, is stored in the IMDG. This includes all bids, wins, campaigns, and performance statistics. Furthermore, the bidders back each other's part of the DB up. Hazelcast is a clustered, fault tolerant customer data platform, that utilizes the standard JAVA Maplike structures. Each bidder is a hazelcast member, and ses the raft consensus protocol to create a fault tolerant database spread across clusters of datagrids. The total size of the database is limited by how many nodes you have running * the memory they have on board. Here's how the system is configured: The overall data base is the sum of the clustered data. In addition, the nodes back each other up, up to a limit of 6 backup nodes for each cluster. Using the Raft consensus protocol, one Bidder is the leader, and handles all Postgres initialization as well as budgeting. Any bidder can handle Campaign manager requests, but in the end they are all handled by the Lead bidder. If that Lead bidder, goes down, the other bidders will elect a new Leader. Budgeting and campaign management activities will not be interrupted. A campaign manager connected to a non-leader would likely not even notice the change in leaders. The IMDG architecture is quite easy to work with. An overview of its features can be seen here: Check out Hazelcast for more information","title":"IMDG"},{"location":"home/overview/#campaign-manager","text":"The campaign manager of RTB4FREE is a REACT based application that will load from any bidder. The campaign manager communicates with the Postgres database to store campaign information. Loading campaigns from Postgres into the Hazelcast cache is done by the Leader bidder running a thread called \"Crosstalk\".","title":"Campaign Manager"},{"location":"home/overview/#postgres-database","text":"A postgres database stores all campaign definitions. The REACT-based campaign manager manipulates the postgres based tables using a well defined SQL schema, through a simple REST API to any bidder in an RTB4FREE cluster. But in reality, you can write your own RTB4FREE campaign manager by defining the appropriate tables in the SQL database and issuing the simple REST based commands to reload campaigns. The schema for the database is located here","title":"Postgres Database"},{"location":"home/overview/#data-management-platform","text":"The Data Management Platform (DMP), the third layer of the DSP. Layer 1 is the Bidder, Layer 2 is the Campaign Manager and Layer 3 is the Data management Platform.","title":"Data Management Platform"},{"location":"home/overview/#features","text":"Integrated with the open source Elasticsearch, Logstash, Kibana (ELK) stack. Search and view RTB transaction log details for requests, bids, wins, pixels, clicks and costs. Search and view RTB application log details for bidder. Out-of-the-box dashboard reports in Kibana. Standard campaign metrics viewable in the Campaign Manager. Create custom reports using Kibana. Open architecture allows integration with your own data analytics infrastructure. Standard data backup and maintenance processes included. Scale up horizontally by deploying clusters for ELK and Kafka. All bidder logs are sent to Kafka, as shown in the following diagram. The system configuration depends on many parameters, including Bidder transaction rate (ie, queries per second). Performance of infrastructure. Available disk space. Historical data retention required. Granularity of data retained. The RTB4FREE architecture can scale to support high data processing rates by horizontally scaling any of the following components: Bidder servers can be added as required to support additional SSP rates. Kafka, which is used for transport of data between bidders and data stores, can be clustered. Kafka consumers (such as Logstash) can be partitioned to share data transfer responsibilities. Elasticsearch can be clustered to accomodate increased data rates or storage requirements. A detailed system configuration can be determined after understanding the detailed on the requirements.","title":"Features"},{"location":"home/overview/#kibana-reports","text":"The RTB log records can be searched by selecting the corresponding index from the dropdown list, then entering a Lucene query in the seach input box. The following record types can be selected. Requests. Requests for bids received from SSP exchanges. Bids. Bid responses to requests. The bidder will respond if a request matches a campaign. Wins. Winning bids from the exchange. Pixels. Impressions that have been served, typically signaled from a pixel image. Clicks. Click events. Postback Events. Additional user defined events, such as video events, mobile application loads, etc. Reasons. If a campaign has not bid on a request, the reason is logged here. This helps tune campaign paramters to increase win rates. RTB Logs. Bidder application log, used to determine health status of bidders. Stats. Bidder usage metrics, used to monitor bidder performance. Kibana contains a report builder that let's you create various display widgets, such as charts, graphs, maps, heat maps, tag clouds, etc. A sample time series graph showing request rate over time is shown below: You can combine various visualizations into a dashboard. This dashboard shows bidder transaction activity as well as bidder performance metrics in a single view. Any data in the logs can be visualized with these tools.","title":"Kibana Reports"},{"location":"home/overview/#campaign-manager-reports","text":"Data logged into Elasticsearch can be made available to users of the Campaign Manger. The following Campaign Manager view shows each campaign's usage using data extracted from Elasticsearch.","title":"Campaign Manager Reports"},{"location":"home/overview/#custom-data-stores","text":"If you have special processing needs, you can build you own app to consume and process the data. An easy method is to build your own Kafka consumer and subscribe to the data topics you wish to process. The open source secor project let's you read any of the RTB logs and store them in cloud storage, such as Amazon S3 or Google Cloud Storage. This option offers unlimited storage without worrying about disk space. You can then download the data for offline data analysis using tools like Hadoop. If you need to process analytics in real time, a custom Kafka consumer can read the log stream and perform real-time analysis and store in your own database.","title":"Custom Data Stores"},{"location":"home/overview/#deployments","text":"For development and testing you can use docker-compose. For production use Kubernetes.","title":"Deployments"},{"location":"home/overview/#docker-compose","text":"","title":"Docker Compose"},{"location":"home/overview/#kubernetes","text":"","title":"Kubernetes"},{"location":"home/overview/#kubernetes-on-aws","text":"","title":"Kubernetes on AWS"},{"location":"home/quickstart/","text":"Building the System \u00b6 Rudder-Transform \u00b6 First step is to make a rudder-transforms docker image. This is not part of the CDP code base but CDP uses these transforms. This is the standard transforms build by Rudderstack. You will need to build the docker container for this. Here are the instructions: A. Either user the C1X cdp-bl code base, or git the standard rudderstack release. B. Change directory to cdp-bl/rudder-transformer C. Run docker build from there $docker build -t c1x/r-transformer . Now on to building the CDP components... JAVA Cluster and Service Components \u00b6 This is the second step of the process. Here we make the Java components of the back-plane of CDP. Make the application using JAVA: $make application React Component Make the UI \u00b6 This is the third step of the process. Makes the react components, if you have made changes to the control program: $make ui Docker Images Dockerize \u00b6 This is the packaging of everything together. Makes the docker image. Do this if you made changes in the application or UI. $make dockerize Docker containers are now built, and you can bring the system up. DOCKER BASED Operations These are the Docker instructions for working with CDP-Cluster, CDP-T Service, Rudderstack Transform Docker Compose Use Docker Compose to run all the components on your system $docker-compose up -d To run everything but CDP-Cluster and CDP-Tserver: $docker-compose -f support.yml up -d Working with Source SENDING EVENTS \u00b6 There is a sample source, with 3 destinations already configured in the database. You can address this source in these sample programs with the value of 1Zg4SLMGY8rGkj7x1anRIWpMMOa. Send a batch of events \u00b6 Assuming you have at least one CDP Cluster running and is mapped to localhost:8080 you can generate an event with: $cd scripts ./generate-event 1Zg4SLMGY8rGkj7x1anRIWpMMOa localhost:8080 Download configuration $cd scripts ./send-download localhost:8080 Upload a configuration $cd scripts ./send-upload filename localhost:8080 Load Representative Sample of All Event Types $cd scripts $./load-msgs 1Zg4SLMGY8rGkj7x1anRIWpMMOa localhost:8080 HANDY DOCKER ACTIVITIES Running Containers $docker ps Attach to a Container Do a docker ps and then use the container-id or name: $docker exec -it /bin/bash Attach to the log \u00b6 Do a docker ps and then use the container-id or name: $docker logs -f Delete an image \u00b6 Do a docker ls first $docker image ls Find the container id $docker image rm --force Correct Checksum Error If docker-compose complains about a checksum after you delete a container do this: $docker-compose rm Connect Intellij Debugger to CDP-Cluster or CDP-T Service: Configuration \u00b6 Each of the docker components has its own set of environment variables. Here is a breakdown on those: Cluster \u00b6 The cluster has environment variables to set up the different sub-systems used by it. The file Docker.env contains the values for the macro substitions below\" environment: OKTADOMAIN: \" ${ OKTADOMAIN } \" TOKENTIMEOUT: \"300\" MESSAGETIMEOUT: \"90\" HTTPS: \"{ $HTTPS }\" ORGANIZATION: \" ${ ORGANIZATION } \" #INDEXPAGE: \"/cdp-control\" INDEXPAGE: \"/control-plane-ui\" PORT: \"8080\" WSPORT: \"8887\" BACKUPCOUNT: \"3\" READBACKUP: \"true\" THREADS: \"128\" KINESIS=\" ${ KINESIS } \" S3=\" ${ S3 } \" JDBC: \" ${ JDBC } \" JDBCDRIVER: \"org.postgresql.Driver\" CONVERSION: \"userId=properties[consumer.upmId]\" MESSAGEFILTER: \"userId != null AND properties[consumer.loginStatus] = 'logged in'\" Overview of the configuration parameters: OKTADOMAIN: This is the issuer domain for verification of session tokens. TOKENTIMEOUT: This is the timeout for api/login sessions, in minutes. Default is 30 mins. Here it is set for 5 hours. MESSAGETIMEOUT: The timeout, in days for messages stored in the cache. Default is forever, here it is set to 90 days. HTTPS: Set to true for HTTPS (self signed cert) or false for HTTP. Default is false. WSPORT: The web services port, set to \"8887\" by default. KINESIS: For Kinesis input of messages to the system. Form kinesis://key=value&key=value... writekey= Writekey to use when ingesting the messages aws_access_key= AWS access key aws_secret_key= AWS secret key stream= The stream name. shard_id= The shard id to use. partition= The partition to use. region= The region to use. record_limit= The number of records to batch on ingest. sleep_time= Number of seconds to wait between reads. records= Number of records to process at a time sleep= Sleep interval between ingests create= Set to true to create the stream if it doesn't exist. S3: For S3 output/input. Form is s3://... aws_access_key= AWS access key aws_secret_key= AWS secret key region= The region to use. bucket= The bucket to use. key= The key to use. Use $datetime to have it use yyyy-mm-dd-hh-mm-ss time format for the key. period= Number of seconds to wait per output to S3. Set to 0 for no buffered output JDBC: This is the POSTGRES access string used by CDP to store messages. CONVERSION - This will promote a subobject to a first level attribute. In this example, userId is null it will be replaced by the value in propertis[consumer.loginStatus]. MESSAGEFILTER - Predicate that screens out unwanted values. In this case we will only process messages where userId is not null and the user is logged in. T-Service The cluster has environment variables to set up the different sub-systems used by it. The file Docker.env contains the values for the macro substitions below\" environment: CLUSTERS: \"cluster1:5701\" TRANSFORMERS: \"http://r-transformer1:9090\" S3: \" ${ S3 } \" KINESIS: \" ${ S3 } \" The following are the meanings - CLUSTERS: Comma separated list of clusters to connect to. - TRANSFORMERS: Location of the dockerized RudderStack destinationtransform processes. - S3: The S3 definition for saving messages and profiles to S3. - KINESIS: The Kinesis specification. THEORY OF OPERATION \u00b6 Like Rudderstack, CDP is a Segment.io replacement. Unlike Rudderstack or Segment, CDP holds all the messages it receives in a clusterable, in memory data grid. Segment has no query capability and Rudderstack queries against Postgres. This means messages received by CDP are queryable at memory speeds. The size of the database is determined by the number of clusters, and queries are distributed, much like Elastic Search. The system is fault tolerant, and uses Raft consensus to determine leader-followers. Partitioning of data across shards is completely automatic. The nodes are designed to be triple-redundant (configurable). Basically, the INPUTS to the system are from APIs (like Segment or Rudderstack) and flow into the Cluster node. Each Cluster node is a Hazelcast cluster. The more nodes, the larger the potential database in memory. The in memory database is queryable using SQL predicates, the queries are distributed across the nodes. The in memory database is automatically saved to Postgres. Metadata is also stored in postgres (like configuration data). The INPUTS from the APIs are Messages (of type track, screen, identify, etc.) These MESSAGEs are stored in a queryable IMDG (in memory data grid). Each MESSAGE flowing into the Cluster becomes a Job. JOBs are placed on a blocking distributed Queue. These queues are known as JOBS, DEADLETTER, COMPLETED and ERRORED. A job on the JOB Queue is waiting to be processed. The T-Service takes a job from the JOBS Queue and places it in the RUNNING Cache. While on the RUNNING Cache, the Job can be queried. Note, JOBS, DEADLETTER, COMPLETED and ERRORED are Queues and cannot be queried - but they can be iterated over. The T-Service takes a JOB from the JOBS Queue (via poll) - Which identifies a message and outputs to N outputs. The T-Service is a Hazelcast CLIENT - it is not part of the cluster. The outputs correspond to Rudderstack Transforms, and are called TASKS). The T-Service handles multiple jobs concurrently using workers. Multiple T-Service components can be run to handle higher loads. The T-Service takes the Job, which identifies the MESSAGE and the output transforms to be used. Using these transforms from the configuration, the corresponding Rudderstack Transformation client is called with an HTTP POST providing the MESSAGE and the configuration for the transformation (Like FB, Kinesis, S3, etc). If the T-Service completes its work, then the Job is placed on the COMPLETED Queue and deleted from the RUNNING Cache. A failure to connect to the Rudderstack transform will be designated as an error. The JOB will be placed in the DEADLETTER Queue for the Cluster to deal with as it sees fit. Note, the RUNNING Cache entries have a timeout. Thus, if the T-Service crashes or exits, any JOBs it was assigned will be deleted from the RUNNING Cache, but will be automatically placed on the DEADLETTER Queue again. The Cluster will take the JOBs on the DEADLETTER Queue and if the retry count is below a configurablethreshold, then it will be placed back onto the JOB Queue. Note, those tasks (transforms) that completed in a previously errored JOB will not be rerun. JOBS that end up on the DEADLETTER QUEUE multiple times will be placed on the ERRORED Queue once the number of jobs has exceeded their retry count. These jobs stay on the ERRORED queue, where they remain indefinitely are unless manually restarted or deleted. Message Flow \u00b6 Receive Input \u00b6 The Cluster-Server receives Segment API messages as HTTP POST messages. These messages have unique IDs, each message loads into the MESSAGES cache, with the id as the key. This is a serializable message and is stored into an ICache. Messages stay in the cache until they are deleted by a predefined eviction strategy. This could be evicted after N elements are in the cache and FIFO eviction occurs or, if configured, it could be based on the time in the cache. In any event, a message written to the cache is always backed up to Postgres database. Messages into Jobs \u00b6 Sending an event to the Cluster will result in the creation of a \"Job\". A job contains a write key, a message id, and keys for destinations. The job is placed on the JOBS IQueue b y the Cluster-Server. T-Service polls the JOBS IQueue, and pops a job to be run off the queue. The Job is updated to running, and it is written back to the RUNNING cache. The T-Service creates a job runner, and executes that with its executor service. When the Job completes, its status is updated and it is then written to the COMPLETED IQueue or the DEADLETTER IQueue, depending on status. A Job has 1 or more destinations. These destinations are Tasks. The configCache is queried to obtain the destination configurations for each task. The job also contains the id of the message and that is retrieved from the MESSAGES cache. With the destination config parameters in hand and the message, an HTTP POST is sent to the Rudderstack Transform service. When the POST returns, the status of the task is updated. How Billing Works \u00b6 Billing is simply a serializable JAVA object with some basic counters. It basically looks like this: public class Billing implements Serializable { Instant timestamp; // The time in UTC format. Long messagesIn; // messages received by the cluster Long messagesOut; // messages sent by the services Long bytesIn; // number of bytes received by the cluster Long bytesOut; // number of bytes transmitted by the service Long queries; // number of queries received Long queryTime; // query time = number-of-ms-for-query * nClusters. Long nClusters; // number of clusters running. Long nServices; // number of services running. } This is stored in a Cache called \"BILLING\". This cache will store the last 10,080 entries. Then they are aged out. Each record is indexed by timestamp - the epoch. Once a minute the Raft leader will create a new Billing record for the clusters and services to update. The Billing records are queryable in memory for up to 7 days. How Journaling to S3 Works \u00b6 How Journaling to Postgres Works \u00b6 QUERIES WITH SQL PREDICATES \u00b6 Standard Hazelcast predicates can be built to handle queries. The most important records to query are of course the messages. Here is a fragment of a track. { \"sentAt\": \"2020-05-06T12:44:04.029Z\" \"channel\": \"web\", \"type\": \"track\", \"context\": { \"app\": { \"build\": \"1.0.0\", \"name\": \"RudderLabs JavaScript SDK\", \"namespace\": \"com.rudderlabs.javascript\", \"version\": \"1.1.1\" }, \"traits\": { \"email\": \"fan.boi@apple.com\" } } \"properties\": { \"my_special_type\": 1000, \"test_123\": \"Hello world\" }, ... Besides properties and traits, the fields are accessed as objects using '.' notation. To access traits or properties, use [] format. Examples from above: context.app.build --> Equals \"1.0.0\" To access properties attribute value for 'my_special_type' use: properties [ my_special_type ] --> Equals 1000 This is because traits and properties have fields that are not defined in the message schema at compile time. These are all user defined fields. Query Simple \u00b6 The predicate to query all track messages would be \"type = track\" Note, the predicate knows the types, so track does not have to be in quotes. Query with AND and OR \u00b6 Here is a predicate for track and alias: \"type = track OR type = alias\" Query With Date Fields. Find all track and web events for anonymousId 123456 and between may 5 and may 10: \"(type = track OR type = alias) AND \\ anonymousId = 123456 AND \\ (timestamp >= 2020-05-05T00:00:00.000Z AND timestamp <= 2020-05-10T23:59:99.999Z)\" Note, all the records will be returned that match. This could be a large number of records, so you have to be prepared to deal with it, or, you can use other predicate types to implement paging and filtering. Query traits and properties \u00b6 Querying the properties and traits) fields is different than all other fields you will encounter. This is because traits and properties can have user defined values. With all other fields you can use the name of the field. With properties and traits, their field names are not set, so, a different query pattern is used. For example, in the above fragment to find a message with the properties field \"my_special_type\" equal to 1000 we would use this: \"properties[my_special_type] contains 1000\" Query Context Traits \u00b6 The attribute \"traits\" may appear at message.traits, or it may appear at message.context.traits. From a user perspective they are treated the same way. However, their implementations are dramatically different. The message.traits uses a distributed IMap evaluator attached to the MESSAGECACHE and PROFILES caches. The 'context' is not at the IMap level, so, it is implemented as a special case by C1X in the Hazelcast source code in file: com.hazelcast.query.impl.QueryableEntry.java at approximately line 127, in the method: extractAttributeValue(String). This is only for support of the messages.context.traits case. It is not a general purpose mapping function.","title":"Quickstart"},{"location":"home/quickstart/#building-the-system","text":"","title":"Building the System"},{"location":"home/quickstart/#rudder-transform","text":"First step is to make a rudder-transforms docker image. This is not part of the CDP code base but CDP uses these transforms. This is the standard transforms build by Rudderstack. You will need to build the docker container for this. Here are the instructions: A. Either user the C1X cdp-bl code base, or git the standard rudderstack release. B. Change directory to cdp-bl/rudder-transformer C. Run docker build from there $docker build -t c1x/r-transformer . Now on to building the CDP components...","title":"Rudder-Transform"},{"location":"home/quickstart/#java-cluster-and-service-components","text":"This is the second step of the process. Here we make the Java components of the back-plane of CDP. Make the application using JAVA: $make application React Component","title":"JAVA Cluster and Service Components"},{"location":"home/quickstart/#make-the-ui","text":"This is the third step of the process. Makes the react components, if you have made changes to the control program: $make ui Docker Images","title":"Make the UI"},{"location":"home/quickstart/#dockerize","text":"This is the packaging of everything together. Makes the docker image. Do this if you made changes in the application or UI. $make dockerize Docker containers are now built, and you can bring the system up. DOCKER BASED Operations These are the Docker instructions for working with CDP-Cluster, CDP-T Service, Rudderstack Transform Docker Compose Use Docker Compose to run all the components on your system $docker-compose up -d To run everything but CDP-Cluster and CDP-Tserver: $docker-compose -f support.yml up -d Working with Source","title":"Dockerize"},{"location":"home/quickstart/#sending-events","text":"There is a sample source, with 3 destinations already configured in the database. You can address this source in these sample programs with the value of 1Zg4SLMGY8rGkj7x1anRIWpMMOa.","title":"SENDING EVENTS"},{"location":"home/quickstart/#send-a-batch-of-events","text":"Assuming you have at least one CDP Cluster running and is mapped to localhost:8080 you can generate an event with: $cd scripts ./generate-event 1Zg4SLMGY8rGkj7x1anRIWpMMOa localhost:8080 Download configuration $cd scripts ./send-download localhost:8080 Upload a configuration $cd scripts ./send-upload filename localhost:8080 Load Representative Sample of All Event Types $cd scripts $./load-msgs 1Zg4SLMGY8rGkj7x1anRIWpMMOa localhost:8080 HANDY DOCKER ACTIVITIES Running Containers $docker ps Attach to a Container Do a docker ps and then use the container-id or name: $docker exec -it /bin/bash","title":"Send a batch of events"},{"location":"home/quickstart/#attach-to-the-log","text":"Do a docker ps and then use the container-id or name: $docker logs -f","title":"Attach to the log"},{"location":"home/quickstart/#delete-an-image","text":"Do a docker ls first $docker image ls Find the container id $docker image rm --force Correct Checksum Error If docker-compose complains about a checksum after you delete a container do this: $docker-compose rm Connect Intellij Debugger to CDP-Cluster or CDP-T Service:","title":"Delete an image"},{"location":"home/quickstart/#configuration","text":"Each of the docker components has its own set of environment variables. Here is a breakdown on those:","title":"Configuration"},{"location":"home/quickstart/#cluster","text":"The cluster has environment variables to set up the different sub-systems used by it. The file Docker.env contains the values for the macro substitions below\" environment: OKTADOMAIN: \" ${ OKTADOMAIN } \" TOKENTIMEOUT: \"300\" MESSAGETIMEOUT: \"90\" HTTPS: \"{ $HTTPS }\" ORGANIZATION: \" ${ ORGANIZATION } \" #INDEXPAGE: \"/cdp-control\" INDEXPAGE: \"/control-plane-ui\" PORT: \"8080\" WSPORT: \"8887\" BACKUPCOUNT: \"3\" READBACKUP: \"true\" THREADS: \"128\" KINESIS=\" ${ KINESIS } \" S3=\" ${ S3 } \" JDBC: \" ${ JDBC } \" JDBCDRIVER: \"org.postgresql.Driver\" CONVERSION: \"userId=properties[consumer.upmId]\" MESSAGEFILTER: \"userId != null AND properties[consumer.loginStatus] = 'logged in'\" Overview of the configuration parameters: OKTADOMAIN: This is the issuer domain for verification of session tokens. TOKENTIMEOUT: This is the timeout for api/login sessions, in minutes. Default is 30 mins. Here it is set for 5 hours. MESSAGETIMEOUT: The timeout, in days for messages stored in the cache. Default is forever, here it is set to 90 days. HTTPS: Set to true for HTTPS (self signed cert) or false for HTTP. Default is false. WSPORT: The web services port, set to \"8887\" by default. KINESIS: For Kinesis input of messages to the system. Form kinesis://key=value&key=value... writekey= Writekey to use when ingesting the messages aws_access_key= AWS access key aws_secret_key= AWS secret key stream= The stream name. shard_id= The shard id to use. partition= The partition to use. region= The region to use. record_limit= The number of records to batch on ingest. sleep_time= Number of seconds to wait between reads. records= Number of records to process at a time sleep= Sleep interval between ingests create= Set to true to create the stream if it doesn't exist. S3: For S3 output/input. Form is s3://... aws_access_key= AWS access key aws_secret_key= AWS secret key region= The region to use. bucket= The bucket to use. key= The key to use. Use $datetime to have it use yyyy-mm-dd-hh-mm-ss time format for the key. period= Number of seconds to wait per output to S3. Set to 0 for no buffered output JDBC: This is the POSTGRES access string used by CDP to store messages. CONVERSION - This will promote a subobject to a first level attribute. In this example, userId is null it will be replaced by the value in propertis[consumer.loginStatus]. MESSAGEFILTER - Predicate that screens out unwanted values. In this case we will only process messages where userId is not null and the user is logged in. T-Service The cluster has environment variables to set up the different sub-systems used by it. The file Docker.env contains the values for the macro substitions below\" environment: CLUSTERS: \"cluster1:5701\" TRANSFORMERS: \"http://r-transformer1:9090\" S3: \" ${ S3 } \" KINESIS: \" ${ S3 } \" The following are the meanings - CLUSTERS: Comma separated list of clusters to connect to. - TRANSFORMERS: Location of the dockerized RudderStack destinationtransform processes. - S3: The S3 definition for saving messages and profiles to S3. - KINESIS: The Kinesis specification.","title":"Cluster"},{"location":"home/quickstart/#theory-of-operation","text":"Like Rudderstack, CDP is a Segment.io replacement. Unlike Rudderstack or Segment, CDP holds all the messages it receives in a clusterable, in memory data grid. Segment has no query capability and Rudderstack queries against Postgres. This means messages received by CDP are queryable at memory speeds. The size of the database is determined by the number of clusters, and queries are distributed, much like Elastic Search. The system is fault tolerant, and uses Raft consensus to determine leader-followers. Partitioning of data across shards is completely automatic. The nodes are designed to be triple-redundant (configurable). Basically, the INPUTS to the system are from APIs (like Segment or Rudderstack) and flow into the Cluster node. Each Cluster node is a Hazelcast cluster. The more nodes, the larger the potential database in memory. The in memory database is queryable using SQL predicates, the queries are distributed across the nodes. The in memory database is automatically saved to Postgres. Metadata is also stored in postgres (like configuration data). The INPUTS from the APIs are Messages (of type track, screen, identify, etc.) These MESSAGEs are stored in a queryable IMDG (in memory data grid). Each MESSAGE flowing into the Cluster becomes a Job. JOBs are placed on a blocking distributed Queue. These queues are known as JOBS, DEADLETTER, COMPLETED and ERRORED. A job on the JOB Queue is waiting to be processed. The T-Service takes a job from the JOBS Queue and places it in the RUNNING Cache. While on the RUNNING Cache, the Job can be queried. Note, JOBS, DEADLETTER, COMPLETED and ERRORED are Queues and cannot be queried - but they can be iterated over. The T-Service takes a JOB from the JOBS Queue (via poll) - Which identifies a message and outputs to N outputs. The T-Service is a Hazelcast CLIENT - it is not part of the cluster. The outputs correspond to Rudderstack Transforms, and are called TASKS). The T-Service handles multiple jobs concurrently using workers. Multiple T-Service components can be run to handle higher loads. The T-Service takes the Job, which identifies the MESSAGE and the output transforms to be used. Using these transforms from the configuration, the corresponding Rudderstack Transformation client is called with an HTTP POST providing the MESSAGE and the configuration for the transformation (Like FB, Kinesis, S3, etc). If the T-Service completes its work, then the Job is placed on the COMPLETED Queue and deleted from the RUNNING Cache. A failure to connect to the Rudderstack transform will be designated as an error. The JOB will be placed in the DEADLETTER Queue for the Cluster to deal with as it sees fit. Note, the RUNNING Cache entries have a timeout. Thus, if the T-Service crashes or exits, any JOBs it was assigned will be deleted from the RUNNING Cache, but will be automatically placed on the DEADLETTER Queue again. The Cluster will take the JOBs on the DEADLETTER Queue and if the retry count is below a configurablethreshold, then it will be placed back onto the JOB Queue. Note, those tasks (transforms) that completed in a previously errored JOB will not be rerun. JOBS that end up on the DEADLETTER QUEUE multiple times will be placed on the ERRORED Queue once the number of jobs has exceeded their retry count. These jobs stay on the ERRORED queue, where they remain indefinitely are unless manually restarted or deleted.","title":"THEORY OF OPERATION"},{"location":"home/quickstart/#message-flow","text":"","title":"Message Flow"},{"location":"home/quickstart/#receive-input","text":"The Cluster-Server receives Segment API messages as HTTP POST messages. These messages have unique IDs, each message loads into the MESSAGES cache, with the id as the key. This is a serializable message and is stored into an ICache. Messages stay in the cache until they are deleted by a predefined eviction strategy. This could be evicted after N elements are in the cache and FIFO eviction occurs or, if configured, it could be based on the time in the cache. In any event, a message written to the cache is always backed up to Postgres database.","title":"Receive Input"},{"location":"home/quickstart/#messages-into-jobs","text":"Sending an event to the Cluster will result in the creation of a \"Job\". A job contains a write key, a message id, and keys for destinations. The job is placed on the JOBS IQueue b y the Cluster-Server. T-Service polls the JOBS IQueue, and pops a job to be run off the queue. The Job is updated to running, and it is written back to the RUNNING cache. The T-Service creates a job runner, and executes that with its executor service. When the Job completes, its status is updated and it is then written to the COMPLETED IQueue or the DEADLETTER IQueue, depending on status. A Job has 1 or more destinations. These destinations are Tasks. The configCache is queried to obtain the destination configurations for each task. The job also contains the id of the message and that is retrieved from the MESSAGES cache. With the destination config parameters in hand and the message, an HTTP POST is sent to the Rudderstack Transform service. When the POST returns, the status of the task is updated.","title":"Messages into Jobs"},{"location":"home/quickstart/#how-billing-works","text":"Billing is simply a serializable JAVA object with some basic counters. It basically looks like this: public class Billing implements Serializable { Instant timestamp; // The time in UTC format. Long messagesIn; // messages received by the cluster Long messagesOut; // messages sent by the services Long bytesIn; // number of bytes received by the cluster Long bytesOut; // number of bytes transmitted by the service Long queries; // number of queries received Long queryTime; // query time = number-of-ms-for-query * nClusters. Long nClusters; // number of clusters running. Long nServices; // number of services running. } This is stored in a Cache called \"BILLING\". This cache will store the last 10,080 entries. Then they are aged out. Each record is indexed by timestamp - the epoch. Once a minute the Raft leader will create a new Billing record for the clusters and services to update. The Billing records are queryable in memory for up to 7 days.","title":"How Billing Works"},{"location":"home/quickstart/#how-journaling-to-s3-works","text":"","title":"How Journaling to S3 Works"},{"location":"home/quickstart/#how-journaling-to-postgres-works","text":"","title":"How Journaling to Postgres Works"},{"location":"home/quickstart/#queries-with-sql-predicates","text":"Standard Hazelcast predicates can be built to handle queries. The most important records to query are of course the messages. Here is a fragment of a track. { \"sentAt\": \"2020-05-06T12:44:04.029Z\" \"channel\": \"web\", \"type\": \"track\", \"context\": { \"app\": { \"build\": \"1.0.0\", \"name\": \"RudderLabs JavaScript SDK\", \"namespace\": \"com.rudderlabs.javascript\", \"version\": \"1.1.1\" }, \"traits\": { \"email\": \"fan.boi@apple.com\" } } \"properties\": { \"my_special_type\": 1000, \"test_123\": \"Hello world\" }, ... Besides properties and traits, the fields are accessed as objects using '.' notation. To access traits or properties, use [] format. Examples from above: context.app.build --> Equals \"1.0.0\" To access properties attribute value for 'my_special_type' use: properties [ my_special_type ] --> Equals 1000 This is because traits and properties have fields that are not defined in the message schema at compile time. These are all user defined fields.","title":"QUERIES WITH SQL PREDICATES"},{"location":"home/quickstart/#query-simple","text":"The predicate to query all track messages would be \"type = track\" Note, the predicate knows the types, so track does not have to be in quotes.","title":"Query Simple"},{"location":"home/quickstart/#query-with-and-and-or","text":"Here is a predicate for track and alias: \"type = track OR type = alias\" Query With Date Fields. Find all track and web events for anonymousId 123456 and between may 5 and may 10: \"(type = track OR type = alias) AND \\ anonymousId = 123456 AND \\ (timestamp >= 2020-05-05T00:00:00.000Z AND timestamp <= 2020-05-10T23:59:99.999Z)\" Note, all the records will be returned that match. This could be a large number of records, so you have to be prepared to deal with it, or, you can use other predicate types to implement paging and filtering.","title":"Query with AND and OR"},{"location":"home/quickstart/#query-traits-and-properties","text":"Querying the properties and traits) fields is different than all other fields you will encounter. This is because traits and properties can have user defined values. With all other fields you can use the name of the field. With properties and traits, their field names are not set, so, a different query pattern is used. For example, in the above fragment to find a message with the properties field \"my_special_type\" equal to 1000 we would use this: \"properties[my_special_type] contains 1000\"","title":"Query traits and properties"},{"location":"home/quickstart/#query-context-traits","text":"The attribute \"traits\" may appear at message.traits, or it may appear at message.context.traits. From a user perspective they are treated the same way. However, their implementations are dramatically different. The message.traits uses a distributed IMap evaluator attached to the MESSAGECACHE and PROFILES caches. The 'context' is not at the IMap level, so, it is implemented as a special case by C1X in the Hazelcast source code in file: com.hazelcast.query.impl.QueryableEntry.java at approximately line 127, in the method: extractAttributeValue(String). This is only for support of the messages.context.traits case. It is not a general purpose mapping function.","title":"Query Context Traits"},{"location":"home/rest/","text":"REST API \u00b6 Endpoint \u00b6 All API messages are JSON payloads to a an HTTP POST, to port 8080 on a single node system, at the URI /api Authorization \u00b6 Get Token \u00b6 Obtain a JSON web token from the system with the 'Get Token' command. A JSON POST message with the form: { type: 3, customer: customer_name, username: user_login_name, password: password_value } Will return the following structure on success: { token: 'the-value-will-be-in-here' } On error it will have an error flag, and a message, like this: { error: true, message: 'no such login' } Be aware that tokens expire, and when they do you will receive the following response: { error: true, message: 'No such token' } You should call get another token to replace the old one. Drop Token \u00b6 When you are logging out of the system, issue a Drop Token command. This will remove the token from the cache and will no longer be usable. { type: 40, token: \"<your-toke-value>\" } If there is no error, the return will look like: { error: false } If error is true, then the token could not be dropped. The attribute 'message' will provide the reason why. Housekeeping \u00b6 Get Shared Resources \u00b6 This returns a summary list of information about the IMDG { token: jwt, type: 16 }; Get User \u00b6 (Not available with Okta authentication) This returns a JSON field with information about a user of the system: { token: jwt, username: username, type: 4 } Add User \u00b6 (Not available with Okta authentication) Adds a user to the system. You need to provide a stringified version of the user object. Here's an example: First, a sample user object: TBD Then transmit the following: { token: jwt, user: JSON.stringify(user), type: 5 } Delete User \u00b6 (Not available with Okta authentication) This command will remove the user from the system: { token: jwt, id: id, type: 6 } Add Company \u00b6 (Not available with Okta authentication) To add a company to the system (or edit), first create the company object like so: TBD Then send the add company command like this: { token: jwt, id: id, type: 9 } Delete Company \u00b6 (Not available with Okta authentication) To delete a company from the system use: { token: jwt, id: id, type: 9 } Retrieve User Objects by Company Id \u00b6 (Not available with Okta authentication) Retrieves all users by their customer id moniker. { token: jwt, customer_id: cid, type: 17 } Returns a list of user objects. List All Companies \u00b6 (Not available with Okta authentication) This command retrieves all the company objects. { token: jwt, type: 18 } The return is a list of company objects. List Active Tokens \u00b6 This command retrieves all the active tokens: { token: jwt, type: 19 } The return is a list of tokens. Clear Trait from Profiles \u00b6 You can clear one or more traits from profiles using the CLEAR API call. This call can remove a trait from a single Profile, or remove a trait from all profiles. Form of command for clearing a trait from a known profile \"97980cfea0067\": { token: jwt, target: \"profile\", item: \"97980cfea0067\", id: \"total_revenue, track_revenue\", type: 39 } The target is set to 'profile'. The 'item' attribute is set to the id of the profile to modify. The id is set to one or more traits to clear from the profile. If id is null, all traits will be removed from the profile. The return 'results' will be the number of traits remaining. To delete a single trait from ALL profiles with that trait, use the following form: { token: jwt, target: \"profile\", id: \"total_revenue\", type: 39 } Not specifying the id of the profile implies all profiles with this trait will be removed. The return 'results' will be the number of profiles affected. If an error occurs 'error' will be set to true and 'message' will contain the appropriate error message. Clear Profile from Segment \u00b6 You can clear one or more profiles from a segment using the CLEAR API call. This call can remove a profile from a single segment, or remove a profile from all segments. Form of command for clearing a profile from segment_123 where profile is \"97980cfea0067\": { token: jwt, target: \"segment\", item: \"segment_123\", id: \"97980cfea0067, 67980cfea0092\", type: 39 } The target is set to 'segment'. The 'item' attribute is set to the name of the segment to modify. The id is set to one or more userId's to clear from the segment. If id is null, all userIds will be removed from the segment. The return 'results' will be 1 for successful deletion. If the segment did not exist, 0 will be returned. If an error occurs, 'error' will be set to true, and 'message' will contain the explanatory message. To delete the profile from ALL segments use the following form: { token: jwt, target: \"segment\", id: \"97980cfea0067\", type: 39 } Not specifying the id of the segment implies all segments with this profile will be removed. The return 'results' will be the number of segments affected. If an error occurs 'error' will be set to true and 'message' will contain the appropriate error message. Clear Audience from Persona \u00b6 You can clear the Audience of user id's from a Persona using the CLEAR API call. This call can remove all t from a Persona. Or, remove all the ids from a persona. In addition, you can remove ids from a Persona->Audience->Segment recursively. Form of command for clearing a profile from a persona where persona is \"my_persona\": { token: jwt, target: \"persona\", item: \"my_persona\", id: \"97980cfea0067\", recursive: true, type: 39 } The target is set to 'persona'. The 'item' attribute is set to the name of the persona to modify. The id is set to one or more userId's to clear from the persona. If id is null, all userIds will be removed from the persona. The optional recursive attribute, if true, will remove the ids from the persona->audience->segment. The return 'results' will be 1 for successful deletion. If the segment did not exist, 0 will be returned. If an error occurs, 'error' will be set to true, and 'message' will contain the explanatory message. To delete a single profile from ALL personas that contain that id use the following form: { token: jwt, target: \"persona\", id: \"97980cfea0067\" type: 39 } To delete the id recursively, set recursive to true. Get Trait Memo \u00b6 This command retrieves a map of the traits, and the number of profiles that have that trait. Example: { token: jwt, type: 40 } The trait object looks like this: { \"ulist_count\": 7, \"logins\": 1, \"name\": 32, \"last\": 1, \"tracker\": 8, \"ulist\": 1, \"track_revenue\": 32, \"plan\": 1, \"email\": 32, \"address\": 1 } In the above example, 32 profiles have a 'name' tait, 1 has a 'ulist' trait, 8 profiles have 'tracker'. Traits \u00b6 Add Computed Trait \u00b6 This adds a computed trait to the system. If you edit a trait, simply call add trait to replace the old one. The different types of traits can be: Event Counter - An Event Counter trait stores a count of an event over a period of time. For example, you can create a trait called number_logins_90_days based on a User Logged In event. You can also use event properties to only specific types of events. User-level examples: Orders Completed Last 30 Days Pricing Page Views Last 30 Days Account-level examples: Total Logins by Account 30 Days Emails Opened by Account 90 Days Aggregation - An aggregation computes a sum, average, minimum, or maximum of a numeric event property. A good example is a sum_cosmetics_revenue_90_days if you\u2019re sending an Order Completed event with a revenue property. In the example we\u2019re refining the revenue even further based on another event property: category = 'cosmetics'. Note that you can only compute an aggregation trait for event properties that have a numeric value. Aggregation types supported: sum highest lowest mean variance geometric mean second moment population variance quadratic mean standard deviation square of the log Most Frequent - A most frequent user-level computed trait will return the most common value for an event property. This is helpful to create traits like preferred_product_viewed or most_commonly_viewed_category that tell you what a user\u2019s preferred product, or content category might be. Note that the most frequent computed trait requires the event property to have been tracked at least twice. In the case of a tie, we return the first alphabetical value. For account-level computed traits, you can also return the most frequent user trait. This is helpful when you want to determine which user has performed an event the most frequently. For example, ou might to return the email of the user in an account most actively viewing your app. User-level examples: Favorite Blog Post Top Purchase Category First - The first user-level trait returns the first event property value we have seen. This is common for creating traits like first_page_visited based on the page name. For accounts, the first computed trait could also return a trait like first_user_signup, to calculate the first user to use your product. User - level examples : First seen timestamp First utm parameter Account - level examples : First email opened First user signup Last- The last trait returns the last event property value we have seen. This is common for creating traits like last_utm_campaign to help you calculate last-touch attribution for paid advertising. User-level examples: Last seen at Last utm parameter Unique List - Unique list computed traits will output a list of unique values for an event property. This is helpful to understand the different types of products or content that a customer or users in an account have interacted with or purchased. Customers are creating traits like unique_product_categories_viewed and sending them to email marketing tools and accessing them through the Profiles API for in-app personalization. Example use cases: Unique products purchased Unique categories Unique games played Unique List Count - Unique list count computed traits will output a count of the unique list of values for an event property. Customers are creating traits like unique_product_categories_viewed_count to understand the variety of products that a customer is viewing. At the account-level, customers are creating traits like unique_visitors_count to calculate the number of unique visitors by ip address. User-level examples: Unique products viewed count Unique categories count The command to add the trait is: { token: jwt, trait: { name: \"name-of-trait\" internal: \"sql where clause here\". ... (other attributes, depends on the trait) } type: 25 } Note, use conditions OR internal, but not both. One or the other is required. Event Counter \u00b6 Counts the number of times an event has been seen. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"allowsMarketing = true\"; \"eventCounter\":{}, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event to count, internal defines the constraint you wish to apply. The eventCounter flag is just a non-empty object, with no other attributes. The response will contain an error field. If errror is true then an errror occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited. First \u00b6 Records the value of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql where clause\", \"first\":{ \"eventProperty\": \"ipAddress\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited. Last \u00b6 Records the last value of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql where clause\", \"last\":{ \"eventProperty\": \"ipAddress\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited. Unique List \u00b6 Records the unique list of values of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql predicate here\", \"uniqueList\":{ \"eventProperty\": \"app.domain\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an errror occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited. Unique List Count \u00b6 Records the count of the unique list of values of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql predicate here\", \"uniqueListCount\":{ \"eventProperty\": \"app.domain\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited. Most Frequent \u00b6 Records the most frequently seen value of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql predicate here\", \"mostFrequent\":{ \"eventProperty\": \"app.domain\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited. Aggregate \u00b6 Applies an aggregation function to a value in traits or properties of a message. This is the only computed trait that can access the message traits values. API Specification: { \"token\" : \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\" , \"trait\" : { \"name\" : \"Sum_revenue\" , \"eventName\" : \"Logged in\" , \"internal\" : \"sql predicate here\" , \"aggregation\" :{ \"eventProperty\" : \"revenue\" , ( properties [ revenue ] ) *** OR *** \"userTrait\" : \"revenue\" , ( traits [ revenue ] ) \"type\" : \"<function-to-apply>\" } , \"lookBack\" : 7 } , \"type\" : 25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. Fucntions: - sum - highest - lowest - mean - var - geomean - smoment - pvar - qmean - std - slogs The response will contain an error field. If errror is true then an errror occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited. SQL \u00b6 Records the return value of an SQL query using values of the message in a select statement API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Real_owner_of_domain\", \"eventName\":\"Logged in\", \"internal\": \"sql predicate to apply\", \"sqlTrait\":{ \"driver\": \"org.postgresql.Driver\", \"jdbc\": \"postgresql://sqldb/postgres?user=aa?password=bb\", \"relation\": \"testable\", \"projection\": \"realowner\", \"predicate\": \"key = {userId}\" }, \"lookBack\":7 }, \"type\":25 } Equivalent to select realowner from testable where key = {userId} Components: - driver is the SQL driver to use - jdbc is the login string - relation is the table name - projection is the column you are looking for. - predicate is the where clause. Can reference message values Using the macro form. Example, for the message userId: Value of realowner will be the value of the computed attribute The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited. Delete Trait \u00b6 To delete a trait from the system: { token: jwt, name: \"<name-of-trait>\", type: 26 } Execute Trait \u00b6 This executes a list of trait using the specified userId: { token: jwt, userId: userId, traits: traits, update: update, type: 27 } The userId defines the profile to execute against. The traits is a list of traits to execute at once. The update field set to true specifies that the profile is to be updated after the execution of the traits. Get Profile \u00b6 This command retrieves a profile object by the userId field. { token: jwt, userId: userId, type: 28 } The return is the profile object that matches that userId. Replay \u00b6 The replay command is used to playback messages through one or more segments or an audience. The results will be an array of id's returned by each segment executed. Example format: { token: jwt, sources: [\"source-1\",\"source-2\",...], segments: [\"segment1\",\"segment2\",...], detach: true|false, audience: \"name-of-audience\", type: 32 } The sources is an array the names of the sources you want to include in the search. If you don't provide any sources then ALL sources will be included. Use the name, not the writeKey. The segments when provided are executed in the order you provide. If you provide no segments you must provide an audience. The audience, if provided will replay the messages through all the segments within that audience. The result will be an array of all the ids that are in that segment. If you provide an audience, don't provide segments. Setting detach will cause the result returned to be a an id to the scratchpad. This id can be used for a query to the scratchpad to return the current status of the running replay. Example: { token: jwt, item: \"SCRATCHPADCACHE\", sql: \"id=<id-returned in the detached replay call\", type: 21 } The return will be an array of message objects, of size 1: { traits: progress: <double-number shows completion> completed: true | false records: <number-of-total records process on completed> ... } If completed is true and progress is less than 100.0 then the call errored. If the call did not error, you can retrieve the results from the segment, or the audience, when it is completed. Real Time Events \u00b6 Real-time events are available from a web socket interface, normally on port 8887 on a single node system. Example: 'ws://localhost:8887?token=<token-value-from-get-token' You can also place the token in the header parameter 'token' too. But you must pass the token, otherwise you will not connect. Immedately upon connecting you will receive a heartbeat message. This will let you know you are connected. Then, once a minute you will receive the heartbeat. Example: Heartbeat \u00b6 { \"stats\": { \"running\": 0, \"tokencache\": 1, \"watch\": 2, \"identity\": 0, \"members\": 0, \"jobs\": 0, \"deadletter\": 0, \"profiles\": 0, \"completed\": 0, \"messagecache\": 0 }, \"time\": \"2020-05-20T15:46:39.119Z\", \"event\": \"heartbeat\" } Real Time Trait \u00b6 The real time trait will send a message on the socket anytime trait has computed a new value and is associated with a profile. You will receive the following key fields: type: This will be equal to 'update_trait'. traitname: This will be the name of the trait that updated. newvalue: This is the new value for the trait on the indicated message.userId. message: The message that caused the trait to excute. Example: { \"newvalue\": 17, \"traitname\": \"count_test_events\" \"type\": \"update_trait\" \"message\": { \"id\": \"c556d2c3-a37e-4cd7-9976-8b0c3cdf2ec4\", \"userId\": \"97980cfea0067\", \"action\": \"track\", \"type\": \"track\", \"event\": \"Test Event\", \"sentAt\": \"2020-05-06T12:44:04.029Z\", \"channel\": \"web\", \"context\": { \"locale\": \"en-GB\", \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36\", \"ip\": \"1.2.3.4\", \"email\": \"user@cdp.com\", \"library\": { \"name\": \"RudderLabs JavaScript SDK\", \"version\": \"1.1.1\" }, \"app\": { \"name\": \"RudderLabs JavaScript SDK\", \"version\": \"1.1.1\", \"namespace\": \"com.rudderlabs.javascript\" }, \"screen\": { \"density\": 1 }, \"traits\": { \"email\": \"user@cdp.com\" }, \"os\": { \"name\": \"\", \"version\": \"\" } }, \"messageId\": \"edb92d20-70ca-4802-b7c9-4769c43a111a\", \"timestamp\": \"2020-05-20T15:54:27.638288Z\", \"properties\": { \"revenue\": 30, \"currency\": \"USD\", \"user_actual_id\": 1234567890 }, \"integrations\": { \"All\": true }, \"originalTimestamp\": \"2020-05-06T12:44:04.028Z\", \"anonymousId\": \"e705b2b9-9d58-4bb1-b6f7-3b9ca2bd98b2\", \"writeKey\": \"1Zg4SLMGY8rGkj7x1anRIWpMMOa\" } } Real Time Segment \u00b6 he real time segment will send a message on the socket anytime profile has been added to a segment. You will receive the following key fields: type: This will be equal to 'update_segment. segment: This will be the name of the segment that updated. newvalue: This is the new profile userId for the segment. Example: { \"newvalue\" : \"93dceei134\" , \"segment\" : \"mynewsegment\" \"type\" : \"update_segment\" } Real Time Audience \u00b6 Real Time Persona \u00b6 Query \u00b6 You can query the different caches using these API access points. TOKENCACHE - the login tokens. MESSAGECACHE - the raw message cache. PROFILES - user profiles. TRAITS - computed traits defined in the system SEGMENTS - segments define SCRATCHPAD Query Keys \u00b6 This query retrieves the key fields from object in the IMDG. { token: jwt, item: cache, sql: pred, type: 20 } The cache field is the name of the IMap to query from. Examples: \"PROFILES\", \"MESSAGECACHE\", \"TRAITS\". The pred field are SQL-like query terms on the stored object format. Example using MESSAGECACHE: \"userId != null AND context.device.name = android\". The return is a list of ids that match the query. To rereive all ids, pass \"\" in the sql field. An example return: Query By Id \u00b6 This will retrieve an object from the IMDG cache based on its key field. { token: jwt, item: cache, key: skey, projections: arrayofnames, type: 22 } The cache field is the name of the IMap to query from. Examples: \"PROFILES\", \"MESSAGECACHE\", \"TRAITS\". The skey is the id of the object. The optional projections is an array of field names to return, instead of the entire object. The field names must exist. If one or more field names do not exist an error will be returned. The projections works for the following caches: TRAITSCACHE, MESSAGECACHE, SEGMENTS, PROFILES, AUDIENCE and PERSONA; all other values are unsupported. The return is the object with that id in the cache. Query \u00b6 This query retrieves the objects in the IMDG cache that match the query predicate. { token : jwt , item : cache , sql : pred , projections : names , type : 21 } The cache field is the name of the IMap to query from. Examples: \"PROFILES\", \"MESSAGECACHE\", \"TRAITS\". The pred field are SQL-like query terms on the stored object format. Example using MESSAGECACHE: \"userId != null AND context.device.name = android\". The projections field, an optional field, is an array of field names you wish to retrieve. The field names must exist, and the pred field cannot be null. The return is a list of messages. To retrieve all the objects, pass \"\" in the sql field. Here is an example command and return from the query of the TRAITS cache: { token : jwt , \"item\" : \"TRAITS\" , \"pred\" : \"\" . \"type\" : 21 } Here is a sample return: { \"type\": 21, \"message\": \"\", \"progress\": 100, \"results\": [ { \"id\": \"3de1ce22-b3f2-43e8-be95-5130bc36a38c\", \"name\": \"tracker\", \"realTime\": true, \"eventName\": \"Test Event\", \"first\": { \"eventProperty\": \"tracker\" }, \"conditions\": [], \"lookBack\": 7, \"timestamp\": \"2020-06-15T21:47:40.308784Z\", \"numUsers\": 0 }, { \"id\": \"1135861a-2142-4ba5-9869-c667e5702da5\", \"name\": \"track_revenue\", \"realTime\": true, \"eventName\": \"Test Event\", \"aggregation\": { \"type\": \"sum\", \"eventProperty\": \"revenue\" }, \"lookBack\": 7, \"timestamp\": \"2020-06-15T21:47:40.331528Z\", \"numUsers\": 0 } ], \"token\": \"20111ATIKX-Q6uQSf3WTU-wXo7ulQd5WS85k5usE3HUdsfp74e30OJi\", \"item\": \"TRAITS\", \"sql\": \"\" } Now, here is an example using projections on the same query to retrieve \"name\" and \"realTime\": Command; { token : jwt , \"item\" : \"TRAITS\" , \"pred\" : \"name!=null\" , \"projections\" : [ \"name\" , \"realTime\" ], \"type\" : 21 } The example return: { \"type\": 21, \"message\": \"\", \"progress\": 100, \"results\": [ { \"name\": \"tracker\", \"realTime\": true }, { \"name\": \"track_revenue\", \"realTime\": true } ], \"token\": \"20111ATIKX-Q6uQSf3WTU-wXo7ulQd5WS85k5usE3HUdsfp74e30OJi\", \"item\": \"TRAITS\", \"sql\": \"name!=null\" } Notice, in this version of the command sql is not \"\". You must put a legal predicate in the sql field that will return what you are looking for. Count User Ids \u00b6 This will retrieve the number of userids in a SEGMENTS, AUDIENCE and PERSONA cache. { token: jwt, item: cache, key: skey, type: 42 } The item field is the name of the IMap to query from, it can be SEGMENTS, AUDIENCE, or PERSONA:. The key is the name of the segment, audience or persona you are querying. Return values - If the item does not support the query (not a SEGMENTS, AUDIENCE or PERSONA), the error field wlll be true. If the key does not exist in the cache, then error will be true. If error is false, the call succeeded, and the size attribute will contain the number of user ids currently stored in the object. Persona \u00b6 Add/Edit Persona \u00b6 { token: jwt, audience: \"name_of_audience\", name: \"name_of_persona\", destination: \"<destination-string> traits: [traitname1, traitname2, ..., traitnameN], format: \"csv-noheader\" | \"csv\" | \"json\" , realTime: setToTrue-for-realtime, type: 36 } The name is the name of the Persona. The audience is the name of the audience to output. The traits array is a list of trait names to be added into the destination. The format determines how the traits would be added to the output. \"csv-noheader\" means comma separated with the elements: Set realTime to make the persona stream to the destinations. The destination is a string defining the output, example: userId,traitname1,traitname2,...,traitnameN actual-user-id,trait1,trait2,...traitN In the case of format is \"csv\", the header is removed. In the case of \"json\": { \"userId\": \"actual-user-id\", \"traitname1\": \"traitvalue1\", \"traitname2\": \"traitvalue2\", \"traitnameN\": \"traitvalueN\" } Delete Persona \u00b6 Delete a persona by name: { token: jwt, name: \"name-of-persona\", type: 37 } Execute Persona \u00b6 { token: jwt, name: \"name-of-persona\", update: setToTrueToUpdate type: 38 } Audience \u00b6 Add/Edit Audience \u00b6 { token: jwt, name: \"name_of_audience\", internal: \"<set-directives\">, realTime: setToTrue-for-realtime, type: 33 } The internal attribute will be used to figure out which segments are being used. The form of the command is in set operations of membership. Grammar supports segmentnames, AND, OR and NOT. Examples: segmentA AND segmentB AND NOT segmentC This means when a userId is added to segmentA, all Audiences containing this segment will be triggered by that entry, and then effectively this means: If segmentA contains userId AND segmentB contains userId AND segmentC does not contain userId then this userId is added to the Audience. Parenthesis can be used to group the operations. NOT can only appear by itself as the first keyword, as in: NOT (segmentA OR segmentB) The following is not a legal expression: segmentA AND segmentB NOT segmentC *** NOT VALID These are valid: segmentA AND segmentB AND NOT segmentC segmentA AND segmentB OR NOT segmentC Note, if you reference a segment that does not exist, then regardless of the userId, that term will resolve to false. If you provide no value for the internal attribute, no realtime capability is possible. Delete Audience \u00b6 { token: jwt, name: \"name-of-audience\", type: 34 } Execute Audience \u00b6 { token: jwt, name: \"name-of-audience\", update: setToTrueToUpdate type: 31 } Segment \u00b6 Add/Edit Segment \u00b6 { token: jwt, name: \"name_of_segment\", conditions: [condition1,condition2, ...], internal: \"sql where clause here\". segment: \"name-of-segment\", type: 29 } Note, use conditions OR internal, but not both. One or the other is required. Using sql attribute allows you to directly provide the operator precedence grammar of queries where clause of the segment. The condition is a 4 part tuple that looks like the following: { apply : < boolean application > , lefthandSide : \"<left-hand-side of the equation>\" , operator : \"<the operator>\" righthandSide : \"<value to test lefthandSide against\" } Apply can be one of the following : - \"\" , implied AND - AND , and with the previous conditions - OR , or with the previous conditions - NOT , ( AND ) not with the previous conditions . The lefthandSide is usually the attribute name . For example in a track message : \"context.library.version\" . The operator can be one of the following : - = - != - > - >= - < - <= - BETWEEN - NOT BETWEEN - CONTAINS - NOT CONTAINS - IN - NOT IN - EXISTS - NOT EXISTS - LIKE - NOT LIKE - STARTS WITH - NOT STARTS WITH - ENDS WITH - NOT ENDS WITH The righthandSide depends on the operator. In the case of EXISTS, or NOT EXISTS it is blank, as it has no meaning in that context. In the case of BETWEEN and NOT BETWEEN, it is 2 element list with AND joing it. Example \"1 AND 2\". In the case of IN, it is a comma separated list in parenthesis. Examples: \"(1,2,3)\", \"('aaa','bbb','ccc')\". Delete Segment \u00b6 { token: jwt, name: \"name-of-segment\", type: 30 } Execute Segment \u00b6 { token: jwt, name: \"name-of-segment\", update: setToTrueToUpdate type: 31 } Add/Edit Audience \u00b6 Delete Audience \u00b6 Misc \u00b6 Put Object in Cache \u00b6 The putobject command allows you to put an object into a cache (such as a profile, persona, audience, segment, trait or message). Most commonly used to edit a profile in place. Example: { token : jwt , cache : \"name-of-cache\" , object : theObject , type : 43 } ``` The cache is the name of the IMDG Queue , example \"PROFILES\" . The object is the JSON object to put into the cache . Depending on the cache , the key is chosen from the object 's attributes. For example, for PROFILES cache the attribute ' userId ' is used as the key . The following is an example of saving a profile : profile = { \"devices\": [], \"anonymousIds\": [ \"e705b2b9-9d58-4bb1-b6f7-3b9ca2bd98b2\", \"507f191e810c19729de860ea\" ], \"messageIds\": [ \"2af227ad-85d3-4355-847d-a45db1761ea5\", \"f05263ee-3926-4d69-9b08-f1729009a103\", \"b684873c-4424-4292-bc05-a4f8ffa113df\", \"16c46ef2-3e92-4915-9ef7-8f7eaca06418\", \"cb72b155-8921-4ced-b9c2-dedecea9cf09\", \"9c33321f-6943-46f6-adb5-dd2cc8102ab8\", \"5fd751c7-e3cc-4cd1-a594-0cc0e26e7ba7\", \"72592039-8819-4500-aba9-8139d4deec0d\" ], \"ips\": [ \"1.2.3.4\", \"8.8.8.8\" ], \"apps\": [ { \"name\": \"RudderLabs JavaScript SDK\", \"build\": \"1.0.0\", \"version\": \"1.1.1\", \"namespace\": \"com.rudderlabs.javascript\" } ], \"userId\": \"97980cfea0067\", \"traits\": { \"revenue\": 30, \"address\": { \"street\": \"6th St\", \"city\": \"San Francisco\", \"state\": \"CA\", \"postalCode\": \"94103\", \"country\": \"USA\" }, \"user_actual_id\": 1234567890, \"name\": \"Peter Gibbons\", \"track_revenue\": 0, \"currency\": \"USD\", \"plan\": \"premium\", \"logins\": 5, \"email\": \"peter@example.com\" }, \"id\": \"84f3dbf6-4022-4287-aa0e-43eff0ca82c1\", \"timestamp\": \"2020-08-07T19:40:07.499471Z\", \"stringStats\": { \"track_revenue\": \"rO0ABXNyADtvcmcuYXBhY2hlLmNvbW1vbnMubWF0aDMuc3RhdC5kZXNjcmlwdGl2ZS5TdW1tYXJ5U3RhdGlzdGljc+Py0otcZHjhAgASSgABbkwAB2dlb01lYW50AEBMb3JnL2FwYWNoZS9jb21tb25zL21hdGgzL3N0YXQvZGVzY3JpcHRpdmUvbW9tZW50L0dlb21ldHJpY01lYW47TAALZ2VvTWVhbkltcGx0AEhMb3JnL2FwYWNoZS9jb21tb25zL21hdGgzL3N0YXQvZGVzY3JpcHRpdmUvU3RvcmVsZXNzVW5pdmFyaWF0ZVN0YXRpc3RpYztMAANtYXh0ADRMb3JnL2FwYWNoZS9jb21tb25zL21hdGgzL3N0YXQvZGVzY3JpcHRpdmUvcmFuay9NYXg7TAAHbWF4SW1wbHEAfgACTAAEbWVhbnQAN0xvcmcvYXBhY2hlL2NvbW1vbnMvbWF0aDMvc3RhdC9kZXNjcmlwdGl2ZS9tb21lbnQvTWVhbjtMAAhtZWFuSW1wbHEAfgACTAADbWludAA0TG9yZy9hcGFjaGUvY29tbW9ucy9tYXRoMy9zdGF0L2Rlc2NyaXB0aXZlL3JhbmsvTWluO0wAB21pbkltcGxxAH4AAkwADHNlY29uZE1vbWVudHQAP0xvcmcvYXBhY2hlL2NvbW1vbnMvbWF0aDMvc3RhdC9kZXNjcmlwdGl2ZS9tb21lbnQvU2Vjb25kTW9tZW50O0wAA3N1bXQAN0xvcmcvYXBhY2hlL2NvbW1vbnMvbWF0aDMvc3RhdC9kZXNjcmlwdGl2ZS9zdW1tYXJ5L1N1bTtMAAdzdW1JbXBscQB+AAJMAAZzdW1Mb2d0AD1Mb3JnL2FwYWNoZS9jb21tb25zL21hdGgzL3N0YXQvZGVzY3JpcHRpdmUvc3VtbWFyeS9TdW1PZkxvZ3M7TAAKc3VtTG9nSW1wbHEAfgACTAAFc3Vtc3F0AEBMb3JnL2FwYWNoZS9jb21tb25zL21hdGgzL3N0YXQvZGVzY3JpcHRpdmUvc3VtbWFyeS9TdW1PZlNxdWFyZXM7TAAJc3Vtc3FJbXBscQB+AAJMAAh2YXJpYW5jZXQAO0xvcmcvYXBhY2hlL2NvbW1vbnMvbWF0aDMvc3RhdC9kZXNjcmlwdGl2ZS9tb21lbnQvVmFyaWFuY2U7TAAMdmFyaWFuY2VJbXBscQB+AAJ4cAAAAAAAAABKc3IAPm9yZy5hcGFjaGUuY29tbW9ucy5tYXRoMy5zdGF0LmRlc2NyaXB0aXZlLm1vbWVudC5HZW9tZXRyaWNNZWFujn9L77lMmYMCAAFMAAlzdW1PZkxvZ3NxAH4AAnhwc3IAO29yZy5hcGFjaGUuY29tbW9ucy5tYXRoMy5zdGF0LmRlc2NyaXB0aXZlLnN1bW1hcnkuU3VtT2ZMb2dz+t047ubViTUCAAJJAAFuRAAFdmFsdWV4cAAAAEpAb3YJD/WwFnEAfgANc3IAMm9yZy5hcGFjaGUuY29tbW9ucy5tYXRoMy5zdGF0LmRlc2NyaXB0aXZlLnJhbmsuTWF4smBPCiPD8l8CAAJKAAFuRAAFdmFsdWV4cAAAAAAAAABKQD4AAAAAAABxAH4AEXNyADVvcmcuYXBhY2hlLmNvbW1vbnMubWF0aDMuc3RhdC5kZXNjcmlwdGl2ZS5tb21lbnQuTWVhbu4DhxRFeuu0AgACWgAJaW5jTW9tZW50TAAGbW9tZW50dAA+TG9yZy9hcGFjaGUvY29tbW9ucy9tYXRoMy9zdGF0L2Rlc2NyaXB0aXZlL21vbWVudC9GaXJzdE1vbWVudDt4cABzcgA9b3JnLmFwYWNoZS5jb21tb25zLm1hdGgzLnN0YXQuZGVzY3JpcHRpdmUubW9tZW50LlNlY29uZE1vbWVudDa2OsGxxcldAgABRAACbTJ4cgA8b3JnLmFwYWNoZS5jb21tb25zLm1hdGgzLnN0YXQuZGVzY3JpcHRpdmUubW9tZW50LkZpcnN0TW9tZW50VNTekKtB+mkCAAREAANkZXZEAAJtMUoAAW5EAARuRGV2eHAAAAAAAAAAAEA+AAAAAAAAAAAAAAAAAEoAAAAAAAAAAAAAAAAAAAAAcQB+ABRzcgAyb3JnLmFwYWNoZS5jb21tb25zLm1hdGgzLnN0YXQuZGVzY3JpcHRpdmUucmFuay5NaW7XK+5rxc1mhQIAAkoAAW5EAAV2YWx1ZXhwAAAAAAAAAEpAPgAAAAAAAHEAfgAZcQB+ABdzcgA1b3JnLmFwYWNoZS5jb21tb25zLm1hdGgzLnN0YXQuZGVzY3JpcHRpdmUuc3VtbWFyeS5TdW2NwqhziTGjRAIAAkoAAW5EAAV2YWx1ZXhwAAAAAAAAAEpAoVgAAAAAAHEAfgAbcQB+AA9xAH4AD3NyAD5vcmcuYXBhY2hlLmNvbW1vbnMubWF0aDMuc3RhdC5kZXNjcmlwdGl2ZS5zdW1tYXJ5LlN1bU9mU3F1YXJlcxRGd9pLErY4AgACSgABbkQABXZhbHVleHAAAAAAAAAASkDwQoAAAAAAcQB+AB1zcgA5b3JnLmFwYWNoZS5jb21tb25zLm1hdGgzLnN0YXQuZGVzY3JpcHRpdmUubW9tZW50LlZhcmlhbmNlgYvOL1sUZ8YCAANaAAlpbmNNb21lbnRaAA9pc0JpYXNDb3JyZWN0ZWRMAAZtb21lbnRxAH4ABnhwAAFxAH4AF3EAfgAf\" }, \"_key\": \"97980cfea0067\" }; cmd = { token: jwt, cache: \"PROFILES\", object: profile, type: 43 }; var response = await axiosInstance.post(\"htto://localhost:8080/api\",JSON.stringify(cmd), { responseType: 'text' }); ... ###Iterate The iterate command allows you to retrieve the objects in IMDG Queues, without removing the items. { token: jwt, queue: queue, limit: limit, type: 23 } ``` The queue is the name of the IMDG Queue, example \"JOBS\". The limit is the number of items to retrieve. To retrieve all objects, don't set the limit. The return is the list of objects in FIFO order. ###Get Replay Messages This command is used to retrieve message for a given source, that match a specific predicate. The source field is the writeKey of the source. To narrow the messages retrieved, use the pred to match on the messages you are looking for, say, by timestamp range. ``` { token: jwt, pred: pred, source: source, type: 24 } Here's an example to retrieve the replay messages for writeKey xxx that matched userIds 12345 23skiddoo, except identify records: { token: jwt, pred: \"(userId = 12345 OR userId = 23skiddoo) AND type != identify\", source: \"xxx\", type: 24 } ``` Get Accounting \u00b6","title":"Rest"},{"location":"home/rest/#rest-api","text":"","title":"REST API"},{"location":"home/rest/#endpoint","text":"All API messages are JSON payloads to a an HTTP POST, to port 8080 on a single node system, at the URI /api","title":"Endpoint"},{"location":"home/rest/#authorization","text":"","title":"Authorization"},{"location":"home/rest/#get-token","text":"Obtain a JSON web token from the system with the 'Get Token' command. A JSON POST message with the form: { type: 3, customer: customer_name, username: user_login_name, password: password_value } Will return the following structure on success: { token: 'the-value-will-be-in-here' } On error it will have an error flag, and a message, like this: { error: true, message: 'no such login' } Be aware that tokens expire, and when they do you will receive the following response: { error: true, message: 'No such token' } You should call get another token to replace the old one.","title":"Get Token"},{"location":"home/rest/#drop-token","text":"When you are logging out of the system, issue a Drop Token command. This will remove the token from the cache and will no longer be usable. { type: 40, token: \"<your-toke-value>\" } If there is no error, the return will look like: { error: false } If error is true, then the token could not be dropped. The attribute 'message' will provide the reason why.","title":"Drop Token"},{"location":"home/rest/#housekeeping","text":"","title":"Housekeeping"},{"location":"home/rest/#get-shared-resources","text":"This returns a summary list of information about the IMDG { token: jwt, type: 16 };","title":"Get Shared Resources"},{"location":"home/rest/#get-user","text":"(Not available with Okta authentication) This returns a JSON field with information about a user of the system: { token: jwt, username: username, type: 4 }","title":"Get User"},{"location":"home/rest/#add-user","text":"(Not available with Okta authentication) Adds a user to the system. You need to provide a stringified version of the user object. Here's an example: First, a sample user object: TBD Then transmit the following: { token: jwt, user: JSON.stringify(user), type: 5 }","title":"Add User"},{"location":"home/rest/#delete-user","text":"(Not available with Okta authentication) This command will remove the user from the system: { token: jwt, id: id, type: 6 }","title":"Delete User"},{"location":"home/rest/#add-company","text":"(Not available with Okta authentication) To add a company to the system (or edit), first create the company object like so: TBD Then send the add company command like this: { token: jwt, id: id, type: 9 }","title":"Add Company"},{"location":"home/rest/#delete-company","text":"(Not available with Okta authentication) To delete a company from the system use: { token: jwt, id: id, type: 9 }","title":"Delete Company"},{"location":"home/rest/#retrieve-user-objects-by-company-id","text":"(Not available with Okta authentication) Retrieves all users by their customer id moniker. { token: jwt, customer_id: cid, type: 17 } Returns a list of user objects.","title":"Retrieve User Objects by Company Id"},{"location":"home/rest/#list-all-companies","text":"(Not available with Okta authentication) This command retrieves all the company objects. { token: jwt, type: 18 } The return is a list of company objects.","title":"List All Companies"},{"location":"home/rest/#list-active-tokens","text":"This command retrieves all the active tokens: { token: jwt, type: 19 } The return is a list of tokens.","title":"List Active Tokens"},{"location":"home/rest/#clear-trait-from-profiles","text":"You can clear one or more traits from profiles using the CLEAR API call. This call can remove a trait from a single Profile, or remove a trait from all profiles. Form of command for clearing a trait from a known profile \"97980cfea0067\": { token: jwt, target: \"profile\", item: \"97980cfea0067\", id: \"total_revenue, track_revenue\", type: 39 } The target is set to 'profile'. The 'item' attribute is set to the id of the profile to modify. The id is set to one or more traits to clear from the profile. If id is null, all traits will be removed from the profile. The return 'results' will be the number of traits remaining. To delete a single trait from ALL profiles with that trait, use the following form: { token: jwt, target: \"profile\", id: \"total_revenue\", type: 39 } Not specifying the id of the profile implies all profiles with this trait will be removed. The return 'results' will be the number of profiles affected. If an error occurs 'error' will be set to true and 'message' will contain the appropriate error message.","title":"Clear Trait from Profiles"},{"location":"home/rest/#clear-profile-from-segment","text":"You can clear one or more profiles from a segment using the CLEAR API call. This call can remove a profile from a single segment, or remove a profile from all segments. Form of command for clearing a profile from segment_123 where profile is \"97980cfea0067\": { token: jwt, target: \"segment\", item: \"segment_123\", id: \"97980cfea0067, 67980cfea0092\", type: 39 } The target is set to 'segment'. The 'item' attribute is set to the name of the segment to modify. The id is set to one or more userId's to clear from the segment. If id is null, all userIds will be removed from the segment. The return 'results' will be 1 for successful deletion. If the segment did not exist, 0 will be returned. If an error occurs, 'error' will be set to true, and 'message' will contain the explanatory message. To delete the profile from ALL segments use the following form: { token: jwt, target: \"segment\", id: \"97980cfea0067\", type: 39 } Not specifying the id of the segment implies all segments with this profile will be removed. The return 'results' will be the number of segments affected. If an error occurs 'error' will be set to true and 'message' will contain the appropriate error message.","title":"Clear Profile from Segment"},{"location":"home/rest/#clear-audience-from-persona","text":"You can clear the Audience of user id's from a Persona using the CLEAR API call. This call can remove all t from a Persona. Or, remove all the ids from a persona. In addition, you can remove ids from a Persona->Audience->Segment recursively. Form of command for clearing a profile from a persona where persona is \"my_persona\": { token: jwt, target: \"persona\", item: \"my_persona\", id: \"97980cfea0067\", recursive: true, type: 39 } The target is set to 'persona'. The 'item' attribute is set to the name of the persona to modify. The id is set to one or more userId's to clear from the persona. If id is null, all userIds will be removed from the persona. The optional recursive attribute, if true, will remove the ids from the persona->audience->segment. The return 'results' will be 1 for successful deletion. If the segment did not exist, 0 will be returned. If an error occurs, 'error' will be set to true, and 'message' will contain the explanatory message. To delete a single profile from ALL personas that contain that id use the following form: { token: jwt, target: \"persona\", id: \"97980cfea0067\" type: 39 } To delete the id recursively, set recursive to true.","title":"Clear Audience from Persona"},{"location":"home/rest/#get-trait-memo","text":"This command retrieves a map of the traits, and the number of profiles that have that trait. Example: { token: jwt, type: 40 } The trait object looks like this: { \"ulist_count\": 7, \"logins\": 1, \"name\": 32, \"last\": 1, \"tracker\": 8, \"ulist\": 1, \"track_revenue\": 32, \"plan\": 1, \"email\": 32, \"address\": 1 } In the above example, 32 profiles have a 'name' tait, 1 has a 'ulist' trait, 8 profiles have 'tracker'.","title":"Get Trait Memo"},{"location":"home/rest/#traits","text":"","title":"Traits"},{"location":"home/rest/#add-computed-trait","text":"This adds a computed trait to the system. If you edit a trait, simply call add trait to replace the old one. The different types of traits can be: Event Counter - An Event Counter trait stores a count of an event over a period of time. For example, you can create a trait called number_logins_90_days based on a User Logged In event. You can also use event properties to only specific types of events. User-level examples: Orders Completed Last 30 Days Pricing Page Views Last 30 Days Account-level examples: Total Logins by Account 30 Days Emails Opened by Account 90 Days Aggregation - An aggregation computes a sum, average, minimum, or maximum of a numeric event property. A good example is a sum_cosmetics_revenue_90_days if you\u2019re sending an Order Completed event with a revenue property. In the example we\u2019re refining the revenue even further based on another event property: category = 'cosmetics'. Note that you can only compute an aggregation trait for event properties that have a numeric value. Aggregation types supported: sum highest lowest mean variance geometric mean second moment population variance quadratic mean standard deviation square of the log Most Frequent - A most frequent user-level computed trait will return the most common value for an event property. This is helpful to create traits like preferred_product_viewed or most_commonly_viewed_category that tell you what a user\u2019s preferred product, or content category might be. Note that the most frequent computed trait requires the event property to have been tracked at least twice. In the case of a tie, we return the first alphabetical value. For account-level computed traits, you can also return the most frequent user trait. This is helpful when you want to determine which user has performed an event the most frequently. For example, ou might to return the email of the user in an account most actively viewing your app. User-level examples: Favorite Blog Post Top Purchase Category First - The first user-level trait returns the first event property value we have seen. This is common for creating traits like first_page_visited based on the page name. For accounts, the first computed trait could also return a trait like first_user_signup, to calculate the first user to use your product. User - level examples : First seen timestamp First utm parameter Account - level examples : First email opened First user signup Last- The last trait returns the last event property value we have seen. This is common for creating traits like last_utm_campaign to help you calculate last-touch attribution for paid advertising. User-level examples: Last seen at Last utm parameter Unique List - Unique list computed traits will output a list of unique values for an event property. This is helpful to understand the different types of products or content that a customer or users in an account have interacted with or purchased. Customers are creating traits like unique_product_categories_viewed and sending them to email marketing tools and accessing them through the Profiles API for in-app personalization. Example use cases: Unique products purchased Unique categories Unique games played Unique List Count - Unique list count computed traits will output a count of the unique list of values for an event property. Customers are creating traits like unique_product_categories_viewed_count to understand the variety of products that a customer is viewing. At the account-level, customers are creating traits like unique_visitors_count to calculate the number of unique visitors by ip address. User-level examples: Unique products viewed count Unique categories count The command to add the trait is: { token: jwt, trait: { name: \"name-of-trait\" internal: \"sql where clause here\". ... (other attributes, depends on the trait) } type: 25 } Note, use conditions OR internal, but not both. One or the other is required.","title":"Add Computed Trait"},{"location":"home/rest/#event-counter","text":"Counts the number of times an event has been seen. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"allowsMarketing = true\"; \"eventCounter\":{}, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event to count, internal defines the constraint you wish to apply. The eventCounter flag is just a non-empty object, with no other attributes. The response will contain an error field. If errror is true then an errror occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited.","title":"Event Counter"},{"location":"home/rest/#first","text":"Records the value of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql where clause\", \"first\":{ \"eventProperty\": \"ipAddress\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited.","title":"First"},{"location":"home/rest/#last","text":"Records the last value of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql where clause\", \"last\":{ \"eventProperty\": \"ipAddress\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited.","title":"Last"},{"location":"home/rest/#unique-list","text":"Records the unique list of values of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql predicate here\", \"uniqueList\":{ \"eventProperty\": \"app.domain\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an errror occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited.","title":"Unique List"},{"location":"home/rest/#unique-list-count","text":"Records the count of the unique list of values of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql predicate here\", \"uniqueListCount\":{ \"eventProperty\": \"app.domain\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited.","title":"Unique List Count"},{"location":"home/rest/#most-frequent","text":"Records the most frequently seen value of the property. API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Count_logins\", \"eventName\":\"Logged in\", \"internal\": \"sql predicate here\", \"mostFrequent\":{ \"eventProperty\": \"app.domain\" }, \"lookBack\":7 }, \"type\":25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited.","title":"Most Frequent"},{"location":"home/rest/#aggregate","text":"Applies an aggregation function to a value in traits or properties of a message. This is the only computed trait that can access the message traits values. API Specification: { \"token\" : \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\" , \"trait\" : { \"name\" : \"Sum_revenue\" , \"eventName\" : \"Logged in\" , \"internal\" : \"sql predicate here\" , \"aggregation\" :{ \"eventProperty\" : \"revenue\" , ( properties [ revenue ] ) *** OR *** \"userTrait\" : \"revenue\" , ( traits [ revenue ] ) \"type\" : \"<function-to-apply>\" } , \"lookBack\" : 7 } , \"type\" : 25 } Attribute name will be the name of the trait. Attribute eventName is the event we are looking for, internal defines the constraint you wish to apply. The first object defines either the eventProperty or userTrait you will be using as the value of the trait. The eventProperty or userTrait (only 1 is allowed) defines the message's attribute that is used as the value. The value of this field can be a a JSON object, and may include an array object. It may also include a qualifier that will compute the array index to use. Examples: \"app.domain\" : Defines a JSON object value under either a user trait or property. \"Brand\" : Defines a JSON object Brand under the properties or traits free form object \"products.0.price\" : Defines products array under proprties or traits, 0th index, price. \"products.{products.any.productId = '12908012'}.price\": Defines the array index where products.N.price is equal to '123456', and retrieves price from that index. Fucntions: - sum - highest - lowest - mean - var - geomean - smoment - pvar - qmean - std - slogs The response will contain an error field. If errror is true then an errror occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited.","title":"Aggregate"},{"location":"home/rest/#sql","text":"Records the return value of an SQL query using values of the message in a select statement API Specification: { \"token\": \"20111ZQtaMtzYYvrMmVv4L7omBbrOZc1b7Mr_-fdvjZ5l4CMcCcMRs8\", \"trait\": { \"name\":\"Real_owner_of_domain\", \"eventName\":\"Logged in\", \"internal\": \"sql predicate to apply\", \"sqlTrait\":{ \"driver\": \"org.postgresql.Driver\", \"jdbc\": \"postgresql://sqldb/postgres?user=aa?password=bb\", \"relation\": \"testable\", \"projection\": \"realowner\", \"predicate\": \"key = {userId}\" }, \"lookBack\":7 }, \"type\":25 } Equivalent to select realowner from testable where key = {userId} Components: - driver is the SQL driver to use - jdbc is the login string - relation is the table name - projection is the column you are looking for. - predicate is the where clause. Can reference message values Using the macro form. Example, for the message userId: Value of realowner will be the value of the computed attribute The response will contain an error field. If errror is true then an error occurred, and the message attribute will contain the error. Response of error = false indicates the trait was added or edited.","title":"SQL"},{"location":"home/rest/#delete-trait","text":"To delete a trait from the system: { token: jwt, name: \"<name-of-trait>\", type: 26 }","title":"Delete Trait"},{"location":"home/rest/#execute-trait","text":"This executes a list of trait using the specified userId: { token: jwt, userId: userId, traits: traits, update: update, type: 27 } The userId defines the profile to execute against. The traits is a list of traits to execute at once. The update field set to true specifies that the profile is to be updated after the execution of the traits.","title":"Execute Trait"},{"location":"home/rest/#get-profile","text":"This command retrieves a profile object by the userId field. { token: jwt, userId: userId, type: 28 } The return is the profile object that matches that userId.","title":"Get Profile"},{"location":"home/rest/#replay","text":"The replay command is used to playback messages through one or more segments or an audience. The results will be an array of id's returned by each segment executed. Example format: { token: jwt, sources: [\"source-1\",\"source-2\",...], segments: [\"segment1\",\"segment2\",...], detach: true|false, audience: \"name-of-audience\", type: 32 } The sources is an array the names of the sources you want to include in the search. If you don't provide any sources then ALL sources will be included. Use the name, not the writeKey. The segments when provided are executed in the order you provide. If you provide no segments you must provide an audience. The audience, if provided will replay the messages through all the segments within that audience. The result will be an array of all the ids that are in that segment. If you provide an audience, don't provide segments. Setting detach will cause the result returned to be a an id to the scratchpad. This id can be used for a query to the scratchpad to return the current status of the running replay. Example: { token: jwt, item: \"SCRATCHPADCACHE\", sql: \"id=<id-returned in the detached replay call\", type: 21 } The return will be an array of message objects, of size 1: { traits: progress: <double-number shows completion> completed: true | false records: <number-of-total records process on completed> ... } If completed is true and progress is less than 100.0 then the call errored. If the call did not error, you can retrieve the results from the segment, or the audience, when it is completed.","title":"Replay"},{"location":"home/rest/#real-time-events","text":"Real-time events are available from a web socket interface, normally on port 8887 on a single node system. Example: 'ws://localhost:8887?token=<token-value-from-get-token' You can also place the token in the header parameter 'token' too. But you must pass the token, otherwise you will not connect. Immedately upon connecting you will receive a heartbeat message. This will let you know you are connected. Then, once a minute you will receive the heartbeat. Example:","title":"Real Time Events"},{"location":"home/rest/#heartbeat","text":"{ \"stats\": { \"running\": 0, \"tokencache\": 1, \"watch\": 2, \"identity\": 0, \"members\": 0, \"jobs\": 0, \"deadletter\": 0, \"profiles\": 0, \"completed\": 0, \"messagecache\": 0 }, \"time\": \"2020-05-20T15:46:39.119Z\", \"event\": \"heartbeat\" }","title":"Heartbeat"},{"location":"home/rest/#real-time-trait","text":"The real time trait will send a message on the socket anytime trait has computed a new value and is associated with a profile. You will receive the following key fields: type: This will be equal to 'update_trait'. traitname: This will be the name of the trait that updated. newvalue: This is the new value for the trait on the indicated message.userId. message: The message that caused the trait to excute. Example: { \"newvalue\": 17, \"traitname\": \"count_test_events\" \"type\": \"update_trait\" \"message\": { \"id\": \"c556d2c3-a37e-4cd7-9976-8b0c3cdf2ec4\", \"userId\": \"97980cfea0067\", \"action\": \"track\", \"type\": \"track\", \"event\": \"Test Event\", \"sentAt\": \"2020-05-06T12:44:04.029Z\", \"channel\": \"web\", \"context\": { \"locale\": \"en-GB\", \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36\", \"ip\": \"1.2.3.4\", \"email\": \"user@cdp.com\", \"library\": { \"name\": \"RudderLabs JavaScript SDK\", \"version\": \"1.1.1\" }, \"app\": { \"name\": \"RudderLabs JavaScript SDK\", \"version\": \"1.1.1\", \"namespace\": \"com.rudderlabs.javascript\" }, \"screen\": { \"density\": 1 }, \"traits\": { \"email\": \"user@cdp.com\" }, \"os\": { \"name\": \"\", \"version\": \"\" } }, \"messageId\": \"edb92d20-70ca-4802-b7c9-4769c43a111a\", \"timestamp\": \"2020-05-20T15:54:27.638288Z\", \"properties\": { \"revenue\": 30, \"currency\": \"USD\", \"user_actual_id\": 1234567890 }, \"integrations\": { \"All\": true }, \"originalTimestamp\": \"2020-05-06T12:44:04.028Z\", \"anonymousId\": \"e705b2b9-9d58-4bb1-b6f7-3b9ca2bd98b2\", \"writeKey\": \"1Zg4SLMGY8rGkj7x1anRIWpMMOa\" } }","title":"Real Time Trait"},{"location":"home/rest/#real-time-segment","text":"he real time segment will send a message on the socket anytime profile has been added to a segment. You will receive the following key fields: type: This will be equal to 'update_segment. segment: This will be the name of the segment that updated. newvalue: This is the new profile userId for the segment. Example: { \"newvalue\" : \"93dceei134\" , \"segment\" : \"mynewsegment\" \"type\" : \"update_segment\" }","title":"Real Time Segment"},{"location":"home/rest/#real-time-audience","text":"","title":"Real Time Audience"},{"location":"home/rest/#real-time-persona","text":"","title":"Real Time Persona"},{"location":"home/rest/#query","text":"You can query the different caches using these API access points. TOKENCACHE - the login tokens. MESSAGECACHE - the raw message cache. PROFILES - user profiles. TRAITS - computed traits defined in the system SEGMENTS - segments define SCRATCHPAD","title":"Query"},{"location":"home/rest/#query-keys","text":"This query retrieves the key fields from object in the IMDG. { token: jwt, item: cache, sql: pred, type: 20 } The cache field is the name of the IMap to query from. Examples: \"PROFILES\", \"MESSAGECACHE\", \"TRAITS\". The pred field are SQL-like query terms on the stored object format. Example using MESSAGECACHE: \"userId != null AND context.device.name = android\". The return is a list of ids that match the query. To rereive all ids, pass \"\" in the sql field. An example return:","title":"Query Keys"},{"location":"home/rest/#query-by-id","text":"This will retrieve an object from the IMDG cache based on its key field. { token: jwt, item: cache, key: skey, projections: arrayofnames, type: 22 } The cache field is the name of the IMap to query from. Examples: \"PROFILES\", \"MESSAGECACHE\", \"TRAITS\". The skey is the id of the object. The optional projections is an array of field names to return, instead of the entire object. The field names must exist. If one or more field names do not exist an error will be returned. The projections works for the following caches: TRAITSCACHE, MESSAGECACHE, SEGMENTS, PROFILES, AUDIENCE and PERSONA; all other values are unsupported. The return is the object with that id in the cache.","title":"Query By Id"},{"location":"home/rest/#query_1","text":"This query retrieves the objects in the IMDG cache that match the query predicate. { token : jwt , item : cache , sql : pred , projections : names , type : 21 } The cache field is the name of the IMap to query from. Examples: \"PROFILES\", \"MESSAGECACHE\", \"TRAITS\". The pred field are SQL-like query terms on the stored object format. Example using MESSAGECACHE: \"userId != null AND context.device.name = android\". The projections field, an optional field, is an array of field names you wish to retrieve. The field names must exist, and the pred field cannot be null. The return is a list of messages. To retrieve all the objects, pass \"\" in the sql field. Here is an example command and return from the query of the TRAITS cache: { token : jwt , \"item\" : \"TRAITS\" , \"pred\" : \"\" . \"type\" : 21 } Here is a sample return: { \"type\": 21, \"message\": \"\", \"progress\": 100, \"results\": [ { \"id\": \"3de1ce22-b3f2-43e8-be95-5130bc36a38c\", \"name\": \"tracker\", \"realTime\": true, \"eventName\": \"Test Event\", \"first\": { \"eventProperty\": \"tracker\" }, \"conditions\": [], \"lookBack\": 7, \"timestamp\": \"2020-06-15T21:47:40.308784Z\", \"numUsers\": 0 }, { \"id\": \"1135861a-2142-4ba5-9869-c667e5702da5\", \"name\": \"track_revenue\", \"realTime\": true, \"eventName\": \"Test Event\", \"aggregation\": { \"type\": \"sum\", \"eventProperty\": \"revenue\" }, \"lookBack\": 7, \"timestamp\": \"2020-06-15T21:47:40.331528Z\", \"numUsers\": 0 } ], \"token\": \"20111ATIKX-Q6uQSf3WTU-wXo7ulQd5WS85k5usE3HUdsfp74e30OJi\", \"item\": \"TRAITS\", \"sql\": \"\" } Now, here is an example using projections on the same query to retrieve \"name\" and \"realTime\": Command; { token : jwt , \"item\" : \"TRAITS\" , \"pred\" : \"name!=null\" , \"projections\" : [ \"name\" , \"realTime\" ], \"type\" : 21 } The example return: { \"type\": 21, \"message\": \"\", \"progress\": 100, \"results\": [ { \"name\": \"tracker\", \"realTime\": true }, { \"name\": \"track_revenue\", \"realTime\": true } ], \"token\": \"20111ATIKX-Q6uQSf3WTU-wXo7ulQd5WS85k5usE3HUdsfp74e30OJi\", \"item\": \"TRAITS\", \"sql\": \"name!=null\" } Notice, in this version of the command sql is not \"\". You must put a legal predicate in the sql field that will return what you are looking for.","title":"Query"},{"location":"home/rest/#count-user-ids","text":"This will retrieve the number of userids in a SEGMENTS, AUDIENCE and PERSONA cache. { token: jwt, item: cache, key: skey, type: 42 } The item field is the name of the IMap to query from, it can be SEGMENTS, AUDIENCE, or PERSONA:. The key is the name of the segment, audience or persona you are querying. Return values - If the item does not support the query (not a SEGMENTS, AUDIENCE or PERSONA), the error field wlll be true. If the key does not exist in the cache, then error will be true. If error is false, the call succeeded, and the size attribute will contain the number of user ids currently stored in the object.","title":"Count User Ids"},{"location":"home/rest/#persona","text":"","title":"Persona"},{"location":"home/rest/#addedit-persona","text":"{ token: jwt, audience: \"name_of_audience\", name: \"name_of_persona\", destination: \"<destination-string> traits: [traitname1, traitname2, ..., traitnameN], format: \"csv-noheader\" | \"csv\" | \"json\" , realTime: setToTrue-for-realtime, type: 36 } The name is the name of the Persona. The audience is the name of the audience to output. The traits array is a list of trait names to be added into the destination. The format determines how the traits would be added to the output. \"csv-noheader\" means comma separated with the elements: Set realTime to make the persona stream to the destinations. The destination is a string defining the output, example: userId,traitname1,traitname2,...,traitnameN actual-user-id,trait1,trait2,...traitN In the case of format is \"csv\", the header is removed. In the case of \"json\": { \"userId\": \"actual-user-id\", \"traitname1\": \"traitvalue1\", \"traitname2\": \"traitvalue2\", \"traitnameN\": \"traitvalueN\" }","title":"Add/Edit Persona"},{"location":"home/rest/#delete-persona","text":"Delete a persona by name: { token: jwt, name: \"name-of-persona\", type: 37 }","title":"Delete Persona"},{"location":"home/rest/#execute-persona","text":"{ token: jwt, name: \"name-of-persona\", update: setToTrueToUpdate type: 38 }","title":"Execute Persona"},{"location":"home/rest/#audience","text":"","title":"Audience"},{"location":"home/rest/#addedit-audience","text":"{ token: jwt, name: \"name_of_audience\", internal: \"<set-directives\">, realTime: setToTrue-for-realtime, type: 33 } The internal attribute will be used to figure out which segments are being used. The form of the command is in set operations of membership. Grammar supports segmentnames, AND, OR and NOT. Examples: segmentA AND segmentB AND NOT segmentC This means when a userId is added to segmentA, all Audiences containing this segment will be triggered by that entry, and then effectively this means: If segmentA contains userId AND segmentB contains userId AND segmentC does not contain userId then this userId is added to the Audience. Parenthesis can be used to group the operations. NOT can only appear by itself as the first keyword, as in: NOT (segmentA OR segmentB) The following is not a legal expression: segmentA AND segmentB NOT segmentC *** NOT VALID These are valid: segmentA AND segmentB AND NOT segmentC segmentA AND segmentB OR NOT segmentC Note, if you reference a segment that does not exist, then regardless of the userId, that term will resolve to false. If you provide no value for the internal attribute, no realtime capability is possible.","title":"Add/Edit Audience"},{"location":"home/rest/#delete-audience","text":"{ token: jwt, name: \"name-of-audience\", type: 34 }","title":"Delete Audience"},{"location":"home/rest/#execute-audience","text":"{ token: jwt, name: \"name-of-audience\", update: setToTrueToUpdate type: 31 }","title":"Execute Audience"},{"location":"home/rest/#segment","text":"","title":"Segment"},{"location":"home/rest/#addedit-segment","text":"{ token: jwt, name: \"name_of_segment\", conditions: [condition1,condition2, ...], internal: \"sql where clause here\". segment: \"name-of-segment\", type: 29 } Note, use conditions OR internal, but not both. One or the other is required. Using sql attribute allows you to directly provide the operator precedence grammar of queries where clause of the segment. The condition is a 4 part tuple that looks like the following: { apply : < boolean application > , lefthandSide : \"<left-hand-side of the equation>\" , operator : \"<the operator>\" righthandSide : \"<value to test lefthandSide against\" } Apply can be one of the following : - \"\" , implied AND - AND , and with the previous conditions - OR , or with the previous conditions - NOT , ( AND ) not with the previous conditions . The lefthandSide is usually the attribute name . For example in a track message : \"context.library.version\" . The operator can be one of the following : - = - != - > - >= - < - <= - BETWEEN - NOT BETWEEN - CONTAINS - NOT CONTAINS - IN - NOT IN - EXISTS - NOT EXISTS - LIKE - NOT LIKE - STARTS WITH - NOT STARTS WITH - ENDS WITH - NOT ENDS WITH The righthandSide depends on the operator. In the case of EXISTS, or NOT EXISTS it is blank, as it has no meaning in that context. In the case of BETWEEN and NOT BETWEEN, it is 2 element list with AND joing it. Example \"1 AND 2\". In the case of IN, it is a comma separated list in parenthesis. Examples: \"(1,2,3)\", \"('aaa','bbb','ccc')\".","title":"Add/Edit Segment"},{"location":"home/rest/#delete-segment","text":"{ token: jwt, name: \"name-of-segment\", type: 30 }","title":"Delete Segment"},{"location":"home/rest/#execute-segment","text":"{ token: jwt, name: \"name-of-segment\", update: setToTrueToUpdate type: 31 }","title":"Execute Segment"},{"location":"home/rest/#addedit-audience_1","text":"","title":"Add/Edit Audience"},{"location":"home/rest/#delete-audience_1","text":"","title":"Delete Audience"},{"location":"home/rest/#misc","text":"","title":"Misc"},{"location":"home/rest/#put-object-in-cache","text":"The putobject command allows you to put an object into a cache (such as a profile, persona, audience, segment, trait or message). Most commonly used to edit a profile in place. Example: { token : jwt , cache : \"name-of-cache\" , object : theObject , type : 43 } ``` The cache is the name of the IMDG Queue , example \"PROFILES\" . The object is the JSON object to put into the cache . Depending on the cache , the key is chosen from the object 's attributes. For example, for PROFILES cache the attribute ' userId ' is used as the key . The following is an example of saving a profile : profile = { \"devices\": [], \"anonymousIds\": [ \"e705b2b9-9d58-4bb1-b6f7-3b9ca2bd98b2\", \"507f191e810c19729de860ea\" ], \"messageIds\": [ \"2af227ad-85d3-4355-847d-a45db1761ea5\", \"f05263ee-3926-4d69-9b08-f1729009a103\", \"b684873c-4424-4292-bc05-a4f8ffa113df\", \"16c46ef2-3e92-4915-9ef7-8f7eaca06418\", \"cb72b155-8921-4ced-b9c2-dedecea9cf09\", \"9c33321f-6943-46f6-adb5-dd2cc8102ab8\", \"5fd751c7-e3cc-4cd1-a594-0cc0e26e7ba7\", \"72592039-8819-4500-aba9-8139d4deec0d\" ], \"ips\": [ \"1.2.3.4\", \"8.8.8.8\" ], \"apps\": [ { \"name\": \"RudderLabs JavaScript SDK\", \"build\": \"1.0.0\", \"version\": \"1.1.1\", \"namespace\": \"com.rudderlabs.javascript\" } ], \"userId\": \"97980cfea0067\", \"traits\": { \"revenue\": 30, \"address\": { \"street\": \"6th St\", \"city\": \"San Francisco\", \"state\": \"CA\", \"postalCode\": \"94103\", \"country\": \"USA\" }, \"user_actual_id\": 1234567890, \"name\": \"Peter Gibbons\", \"track_revenue\": 0, \"currency\": \"USD\", \"plan\": \"premium\", \"logins\": 5, \"email\": \"peter@example.com\" }, \"id\": \"84f3dbf6-4022-4287-aa0e-43eff0ca82c1\", \"timestamp\": \"2020-08-07T19:40:07.499471Z\", \"stringStats\": { \"track_revenue\": \"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\" }, \"_key\": \"97980cfea0067\" }; cmd = { token: jwt, cache: \"PROFILES\", object: profile, type: 43 }; var response = await axiosInstance.post(\"htto://localhost:8080/api\",JSON.stringify(cmd), { responseType: 'text' }); ... ###Iterate The iterate command allows you to retrieve the objects in IMDG Queues, without removing the items. { token: jwt, queue: queue, limit: limit, type: 23 } ``` The queue is the name of the IMDG Queue, example \"JOBS\". The limit is the number of items to retrieve. To retrieve all objects, don't set the limit. The return is the list of objects in FIFO order. ###Get Replay Messages This command is used to retrieve message for a given source, that match a specific predicate. The source field is the writeKey of the source. To narrow the messages retrieved, use the pred to match on the messages you are looking for, say, by timestamp range. ``` { token: jwt, pred: pred, source: source, type: 24 } Here's an example to retrieve the replay messages for writeKey xxx that matched userIds 12345 23skiddoo, except identify records: { token: jwt, pred: \"(userId = 12345 OR userId = 23skiddoo) AND type != identify\", source: \"xxx\", type: 24 } ```","title":"Put Object in Cache"},{"location":"home/rest/#get-accounting","text":"","title":"Get Accounting"},{"location":"home/theory/","text":"Theory of Operation \u00b6 The bidding engine is based on OpenRTB 2.8 and is written in JAVA running on the 1.11 Java virtual machine. It is a set of dockerized containers employing the RTB bidder, Postgres, Kafka, Zookeeper, and the ELK stack. Not included in the stack but a key part of the infrastructure is your own load balancer. Load your SSL certificates in your load balancer, not the bidder. Internally the bidder is a JAVA program that receives RTB bid requests via HTTP (we use JETTY). The bid requests are converted to JSON objects (we use JACKSON for this). Bid requests are compared against campaigns stored in memory (we use Hazelcast IMap to store all the campaigns in shared memory). When requests come in the bids are selected against the campaigns, if one of the campaigns mathes then a bid is formed and stored in the shared memory (Hazelcast IMap) then the bid is trasmitted back to the SSP. If you win the auction a win notification is sent to the bidder. When the ad is shown in the user's browser a pixel fire is received, or if it is a video, various video status messages may be sent to the bidder. The bidder sends statistical samples of all bid requests to a Kafka channel for use by the ELK stack. Win notifications are also sent on a Kafka channel to the ELK stack so that the campaign's performance may be monitored and to perform budgeting. Pixel and video fires are also sent on a Kafka channel back to the ELK stack. The campaign manager is a REACT application that interfaces wih the Postgres database through a simple API. It is possible to interface your own campaign manager to the Postgres DB through a well defined schema. A few REST calls are needed to control loading and unloading campaigns, but it is all pretty straightforward. The bidders also act as web servers and you load the SPA from any of the bidders, and also, can send the REST API calls to any running bidder. The campaign manager and the Postgres database are designed to support multi-tenancy. The bidders form a cluster of bidders. The bidders share memory so that they see all the bids outstanding, all the campaigns, and all the frequency capping data. This is all handled with Hazelcast IMaps, using the In Memory Data Grid to store all the relevant shared data. Only one bidder runs a process called \"Crosstalk\". Crosstalk is the process that connects the Postgres database to the campaigns stored in the bidders. So while all bidders can read the database. All writeable operations go through the Lead bidder. The lead bidder is elected using the Raft Consensus Protoco. Should the lead bidder go down, a new leader will be elected automatically. Budgeting is stored in the Postgres database, updated on the minute by Crosstalk, running in the Lead bidder. Once budget limits have been reached, Crosstalk will unload those campaigns that have crossed the threshold. Remember, Crosstalk runs in the Leader bidder. All analytics are handled by the ELK stack. Kafka channels route bid requests, bids, wins, pixel fires, video fires, and win notifications back to Logstash which loads the data into Elastic. Kibana is used to create views and dashboards. Bidding Engine \u00b6 IMDG \u00b6 Many different technologies have been tried for maintaining state information between multiple bidders and the campaign manager: Redis and Aerospike being most notable. In the end tbese technologies brought more bloat than value. What we really need is a cache between the bidders, and the campaign manager that used familiar JAVA structures, like Maps. That's exactly what Hazelcast offers for us, with its \"In Memory Data Grid\" IMDG. All of the shared data needed by the bidders, is stored in the IMDG. This includes all bids, wins, campaigns, and performance statistics. Furthermore, the bidders back each other's part of the DB up. Hazelcast is a clustered, fault tolerant customer data platform, that utilizes the standard JAVA Maplike structures. Each bidder is a hazelcast member, and ses the raft consensus protocol to create a fault tolerant database spread across clusters of datagrids. The total size of the database is limited by how many nodes you have running * the memory they have on board. Here's how the system is configured: The overall data base is the sum of the clustered data. In addition, the nodes back each other up, up to a limit of 6 backup nodes for each cluster. Using the Raft consensus protocol, one Bidder is the leader, and handles all Postgres initialization as well as budgeting. Any bidder can handle Campaign manager requests, but in the end they are all handled by the Lead bidder. If that Lead bidder, goes down, the other bidders will elect a new Leader. Budgeting and campaign management activities will not be interrupted. A campaign manager connected to a non-leader would likely not even notice the change in leaders. The IMDG architecture is quite easy to work with. An overview of its features can be seen here: Check out Hazelcast for more information Crosstalk \u00b6 Crosstalk is a processing thread that runs inside the RTB bidder. This process has a REST API that allows you to create and manipulate campaigns, creatives, etc using HTTP POSTs (and stored n a Postgres database. Crosstalk also runs a budgeting process once a minute that connects to Elastic to retrieve campaign performance information and writes it to the IMDG and to the Postgres database. Note, Crosstalk runs in all the bidders, but budgeting is handled by the Leader bidder. API \u00b6 Frequency Capping \u00b6 Frequency capping, typically tied to IP addresses (but not only way) is defined by the campaign manager and is stored in Postgres database and the IMDG. The frequency caps are created for each winning bid as per the campaign and these caps create counters in the IMDG and are visible to all bidders. Budgeting \u00b6 Budgeting is estabished by the campaign manager and these budgets are stored in Postgres and in the IMDG, visible to all the Crosstalk threads within the bidders. Once a minute the Lead bidder's Crosstalk thread queries Elastic for the value of the current campaigns. Budgeting may be established for campaign and creative levels. After Crosstalk retrieves the budget it looks to see if any running campaigns and/or creatives have exceeded their budgets. If so, the campaigns are unloaded (in the case of just a creative is at budget, the creative is unloaded). Campaign Manager \u00b6 The campaign manager of RTB4FREE is a REACT based application that will load from any bidder. The campaign manager communicates with the Postgres database to store campaign information. Loading campaigns from Postgres into the Hazelcast cache is done by the Leader bidder running a thread called \"Crosstalk\". SPA \u00b6 Company/User Profiles \u00b6 Targets \u00b6 Creatives \u00b6 Capaigns \u00b6 Rules \u00b6 Simulator \u00b6 Postgres Database \u00b6 Overview \u00b6 A Postgres database stores all campaign definitions. Note the appropriate tables are partitioned by customer_id to support multi-tenant operations. The REACT-based campaign manager manipulates the postgres based tables using a well defined SQL schema, through a simple REST API to any bidder in an RTB4FREE cluster. But in reality, you can write your own RTB4FREE campaign manager by accessing and updating the appropriate tables in the SQL database and issuing the simple REST based commands to reload campaigns. To create a viable campaign you will have one row for a campaign in the \u201ccampaigns\u201d table. There will be a single row in the \u201ctargets\u201d table for each campaign. The target_id field in the campaigns table is the value of the id in the targets table. In either, or both \u201cbanners\u201d table and \u201cbanner_videos\u201d table there will be at least one row in the combination of both of these tables of a campaign_id field equal to the id in the campaigns table. In the \u201ccampaign_rtb_standards\u201d table there will be zero or more rows with the campaign_id field equal to the id field in the \u201ccampaigns\u201d table. The rtb_standard_id field is equal to the id field in the \u201crtb_standards\u201d table. This is how custom constraints are attached at the campaign level. In the \u201cbanner_rtb_standards\u201d table there will be zero or more rows with the campaign_id field equal to the id field in the \u201cbanners\u201d table. The rtb_standard_id field is equal to the id field in the \u201crtb_standards\u201d table. This is how custom constraints are attached at the banner creative level. In the \u201cvideo_rtb_standards\u201d table there will be zero or more rows with the video_id field equal to the id field in the \u201cbanners_video\u201d table. The rtb_standards_id field is equal to the id field in the \u201crtb_standards\u201d table. This is how custom constraints are attached at the video creative level. ERD \u00b6 When in doubt, look at a campaign created with the RTB4FREE campaign manager to see what the corresponding values look like. The schema for the database is located here Data Management Platform \u00b6 The Data Management Platform (DMP), the third layer of the DSP. Layer 1 is the Bidder, Layer 2 is the Campaign Manager and Layer 3 is the Data management Platform.","title":"Theory"},{"location":"home/theory/#theory-of-operation","text":"The bidding engine is based on OpenRTB 2.8 and is written in JAVA running on the 1.11 Java virtual machine. It is a set of dockerized containers employing the RTB bidder, Postgres, Kafka, Zookeeper, and the ELK stack. Not included in the stack but a key part of the infrastructure is your own load balancer. Load your SSL certificates in your load balancer, not the bidder. Internally the bidder is a JAVA program that receives RTB bid requests via HTTP (we use JETTY). The bid requests are converted to JSON objects (we use JACKSON for this). Bid requests are compared against campaigns stored in memory (we use Hazelcast IMap to store all the campaigns in shared memory). When requests come in the bids are selected against the campaigns, if one of the campaigns mathes then a bid is formed and stored in the shared memory (Hazelcast IMap) then the bid is trasmitted back to the SSP. If you win the auction a win notification is sent to the bidder. When the ad is shown in the user's browser a pixel fire is received, or if it is a video, various video status messages may be sent to the bidder. The bidder sends statistical samples of all bid requests to a Kafka channel for use by the ELK stack. Win notifications are also sent on a Kafka channel to the ELK stack so that the campaign's performance may be monitored and to perform budgeting. Pixel and video fires are also sent on a Kafka channel back to the ELK stack. The campaign manager is a REACT application that interfaces wih the Postgres database through a simple API. It is possible to interface your own campaign manager to the Postgres DB through a well defined schema. A few REST calls are needed to control loading and unloading campaigns, but it is all pretty straightforward. The bidders also act as web servers and you load the SPA from any of the bidders, and also, can send the REST API calls to any running bidder. The campaign manager and the Postgres database are designed to support multi-tenancy. The bidders form a cluster of bidders. The bidders share memory so that they see all the bids outstanding, all the campaigns, and all the frequency capping data. This is all handled with Hazelcast IMaps, using the In Memory Data Grid to store all the relevant shared data. Only one bidder runs a process called \"Crosstalk\". Crosstalk is the process that connects the Postgres database to the campaigns stored in the bidders. So while all bidders can read the database. All writeable operations go through the Lead bidder. The lead bidder is elected using the Raft Consensus Protoco. Should the lead bidder go down, a new leader will be elected automatically. Budgeting is stored in the Postgres database, updated on the minute by Crosstalk, running in the Lead bidder. Once budget limits have been reached, Crosstalk will unload those campaigns that have crossed the threshold. Remember, Crosstalk runs in the Leader bidder. All analytics are handled by the ELK stack. Kafka channels route bid requests, bids, wins, pixel fires, video fires, and win notifications back to Logstash which loads the data into Elastic. Kibana is used to create views and dashboards.","title":"Theory of Operation"},{"location":"home/theory/#bidding-engine","text":"","title":"Bidding Engine"},{"location":"home/theory/#imdg","text":"Many different technologies have been tried for maintaining state information between multiple bidders and the campaign manager: Redis and Aerospike being most notable. In the end tbese technologies brought more bloat than value. What we really need is a cache between the bidders, and the campaign manager that used familiar JAVA structures, like Maps. That's exactly what Hazelcast offers for us, with its \"In Memory Data Grid\" IMDG. All of the shared data needed by the bidders, is stored in the IMDG. This includes all bids, wins, campaigns, and performance statistics. Furthermore, the bidders back each other's part of the DB up. Hazelcast is a clustered, fault tolerant customer data platform, that utilizes the standard JAVA Maplike structures. Each bidder is a hazelcast member, and ses the raft consensus protocol to create a fault tolerant database spread across clusters of datagrids. The total size of the database is limited by how many nodes you have running * the memory they have on board. Here's how the system is configured: The overall data base is the sum of the clustered data. In addition, the nodes back each other up, up to a limit of 6 backup nodes for each cluster. Using the Raft consensus protocol, one Bidder is the leader, and handles all Postgres initialization as well as budgeting. Any bidder can handle Campaign manager requests, but in the end they are all handled by the Lead bidder. If that Lead bidder, goes down, the other bidders will elect a new Leader. Budgeting and campaign management activities will not be interrupted. A campaign manager connected to a non-leader would likely not even notice the change in leaders. The IMDG architecture is quite easy to work with. An overview of its features can be seen here: Check out Hazelcast for more information","title":"IMDG"},{"location":"home/theory/#crosstalk","text":"Crosstalk is a processing thread that runs inside the RTB bidder. This process has a REST API that allows you to create and manipulate campaigns, creatives, etc using HTTP POSTs (and stored n a Postgres database. Crosstalk also runs a budgeting process once a minute that connects to Elastic to retrieve campaign performance information and writes it to the IMDG and to the Postgres database. Note, Crosstalk runs in all the bidders, but budgeting is handled by the Leader bidder.","title":"Crosstalk"},{"location":"home/theory/#api","text":"","title":"API"},{"location":"home/theory/#frequency-capping","text":"Frequency capping, typically tied to IP addresses (but not only way) is defined by the campaign manager and is stored in Postgres database and the IMDG. The frequency caps are created for each winning bid as per the campaign and these caps create counters in the IMDG and are visible to all bidders.","title":"Frequency Capping"},{"location":"home/theory/#budgeting","text":"Budgeting is estabished by the campaign manager and these budgets are stored in Postgres and in the IMDG, visible to all the Crosstalk threads within the bidders. Once a minute the Lead bidder's Crosstalk thread queries Elastic for the value of the current campaigns. Budgeting may be established for campaign and creative levels. After Crosstalk retrieves the budget it looks to see if any running campaigns and/or creatives have exceeded their budgets. If so, the campaigns are unloaded (in the case of just a creative is at budget, the creative is unloaded).","title":"Budgeting"},{"location":"home/theory/#campaign-manager","text":"The campaign manager of RTB4FREE is a REACT based application that will load from any bidder. The campaign manager communicates with the Postgres database to store campaign information. Loading campaigns from Postgres into the Hazelcast cache is done by the Leader bidder running a thread called \"Crosstalk\".","title":"Campaign Manager"},{"location":"home/theory/#spa","text":"","title":"SPA"},{"location":"home/theory/#companyuser-profiles","text":"","title":"Company/User Profiles"},{"location":"home/theory/#targets","text":"","title":"Targets"},{"location":"home/theory/#creatives","text":"","title":"Creatives"},{"location":"home/theory/#capaigns","text":"","title":"Capaigns"},{"location":"home/theory/#rules","text":"","title":"Rules"},{"location":"home/theory/#simulator","text":"","title":"Simulator"},{"location":"home/theory/#postgres-database","text":"","title":"Postgres Database"},{"location":"home/theory/#overview","text":"A Postgres database stores all campaign definitions. Note the appropriate tables are partitioned by customer_id to support multi-tenant operations. The REACT-based campaign manager manipulates the postgres based tables using a well defined SQL schema, through a simple REST API to any bidder in an RTB4FREE cluster. But in reality, you can write your own RTB4FREE campaign manager by accessing and updating the appropriate tables in the SQL database and issuing the simple REST based commands to reload campaigns. To create a viable campaign you will have one row for a campaign in the \u201ccampaigns\u201d table. There will be a single row in the \u201ctargets\u201d table for each campaign. The target_id field in the campaigns table is the value of the id in the targets table. In either, or both \u201cbanners\u201d table and \u201cbanner_videos\u201d table there will be at least one row in the combination of both of these tables of a campaign_id field equal to the id in the campaigns table. In the \u201ccampaign_rtb_standards\u201d table there will be zero or more rows with the campaign_id field equal to the id field in the \u201ccampaigns\u201d table. The rtb_standard_id field is equal to the id field in the \u201crtb_standards\u201d table. This is how custom constraints are attached at the campaign level. In the \u201cbanner_rtb_standards\u201d table there will be zero or more rows with the campaign_id field equal to the id field in the \u201cbanners\u201d table. The rtb_standard_id field is equal to the id field in the \u201crtb_standards\u201d table. This is how custom constraints are attached at the banner creative level. In the \u201cvideo_rtb_standards\u201d table there will be zero or more rows with the video_id field equal to the id field in the \u201cbanners_video\u201d table. The rtb_standards_id field is equal to the id field in the \u201crtb_standards\u201d table. This is how custom constraints are attached at the video creative level.","title":"Overview"},{"location":"home/theory/#erd","text":"When in doubt, look at a campaign created with the RTB4FREE campaign manager to see what the corresponding values look like. The schema for the database is located here","title":"ERD"},{"location":"home/theory/#data-management-platform","text":"The Data Management Platform (DMP), the third layer of the DSP. Layer 1 is the Bidder, Layer 2 is the Campaign Manager and Layer 3 is the Data management Platform.","title":"Data Management Platform"},{"location":"home/userguides/","text":"Overview \u00b6 The Campaign Manager, the second layer of the DSP. Layer 1 is the Bidder, Layer 2 is the Campaign Manager and Layer 3 is the Data Management Platform. Features at a glance: REACT Web based. Supports Banner, Video, Audio and Native Ads. Provides Day Parting service. Employs Open, Hourly, Daily and Total budgeting. Easy to use targetting. Build your own specialized rules if standard targetting is not sufficient. Docker Microservices Architecture. Built with REACT. Multi-tenant. A campaign is the offering of one or more creatives over a certain time period, and with specific targets. We use the term target to mean constraining rules that filter out those id requests that match your target market. When we say reative we mean a banner ad, a video or a native ad. Additionally, you can use the Campaign Manager to review the the running status of your campaign, and to start and stop campaigns. The process to create a new campaign is: Select the Campaigns icon and define the basic campaign parameters Save the campaign. Select the Banners or Video icon and define the creative parameters. Select the campaign you created in step 1 to assign the creative to your campaign. Save the creative. Select the Target icon and define the target parameters. Save the target. Return to the Campaign edit page, and select the Target that you created. Saving the Campaign will load it in the bidder. Accessing \u00b6 To access the RTB4FREE Campaign Manager point your browser to http://localhost:8100/campaigns. The following screen should appear: On initial build, this user ben.faul@rtb4free.com is already configured. Use the following values to login the first time. Org: This should be rtb4free. This is the company id of the user, the database support multi-tenancy so multiple orgs can be supported. User: ben.faul@rtb4free.com - This is the user already configured. Password: zulu - This is the initial password. Server: localhost:7379. Point this to any bidder at this port id. This is the API port. Dashboard \u00b6 On initially logging in , you will be at the dashboard, like this: The Instances show you how many bidders are running in this cluster. In this case only 1. The Running Campaigns shows how many campaigns are loaded into the cluster. In this case there are none. Bidding \u00b6 To send bids to the SSPs you need viable campaigns. A campaign is the offering of one or more creatives over a certain time period, and with specific targets. We use the term target to mean common constraints that filter out those bid requests to match your target market. When we say creative we mean a banner ad, a video or a native ad. Campaigns \u00b6 Pressing the Campaign icon on the blue dashboard will bring up the following screen that shows existing campaigns, in this case there are no campaigns yet: Pressing the red New button will allow us to create a new campaign: And scrolling down a little: The following fields are set forth with their meanings: SQL idhis is a db id, it is nt editable. Name This is the name of the campaign. It must be a unique name. REQUIRED . Ad Domain: This is the advertisers root domain. REQUIRED Fraud Suppression: This indicates if built in fraud suppression is used, but no effect if not enabled at the system level. Bidder status: Whether this is an actve campaign. Set to online to bid (but will only go online if it is a viable campaign). Creatives: This list will show what creatives are assigned to this campaign. Not viable without creatives Start: The start date and time for campaign activation REQUIRED Stop: The stop time of campaign activation. REQUIRED Spend rate: Regardless of budget, maximum spend rate per minute of a campaign. Setting nothing or 0 means no limit. Bidding Region: Choose a region where this campaign is to be bid in, REQURED Exchanges: Choose none or multiple exchanges (if they are connected). Selecting none will enable all. Target: Choose a target to attach to the campaign Not viable without a target Rules: Choose 0 or more defined rules to attach to the campaign. A rule is a specialized constraint. Total Budget: Total amount of US dollars to allocate to this campaign. Setting 0 means no budget. Daily Budget: Amount campaign is to spend per day. Setting 0 means no daily constraint. Hourly Budget: Amount campaugn is to spend per hour. Setting 0 means no set hourly constraint. Frequency Specification: Defines the RTB spec JSON name to key on. For example device.ip Dayparting: Sets a calendar of when to spend by days of the week and by time. Push the purple SAVE button to save the campaign. The following is a sample campaign. Creating a campaign named \"Test-Campaign-1\" with no targets or creatives creates the campaign byt it will be offline as indicated by the grey stripe: Pressing the blue Report button will tell you why the campaign is offline. In this case \"No attached creatives. Marked offline\" is displayed in the popup after Report is pressed. Fields with \"*\" indicate a required field. Most of the fields are self-explanatory, with the exception of Region, and Rules. A Region is a bidding region. It is the name given to a group of bidders controlled by Crosstalk. Only one Region can be specified. If you are set up to bid in the US, then use US as the region. If in Asia, use AP, and if in Europe use EU. Rules are specific constraints applied to the campaign gathered from the bid requests and are discussed further in the documentation. Once you have a campaign created, you can add Banner and Video, Audio and Native Ad creatives. Targets \u00b6 The next step is to create a target. Pressing the Targets icon in the blue field causes the following to appear: Pressing the red New button will cause the new target window to appear: Here is a description of the fields and what they mean: SQL ID: The database key id, not editable. Ad id: This is the target's name REQUIRED . Domain Values: CCC. Use Big Data Set for Domain Values. Type: Select WHITELIST to make sure the domain values are present, otherwise BLACKLIST to exclude from bidding the value is present. Country: Put the country names in by 3 character ISO names. With nothing specified all countries are valid. Geo Latitiude: Put the GPS latitude here to create a fence. None means no fence. Geo Longitude: Put the GPS longitude here to create a fence. None meand no fence. Geo Range: Inclusive circle in Km around lat/lon for the fence. Note, for multiple fences use a rule. Carrier: Name of the carrier to key on. Case sensitive Comma separated list. Operating System: Name of OS. Case Sensitive Comma separated list. Make: A single make is allowed. Model: Models supported, one per line. Device types: Choose one or more. Choosing none selects all. IAB Whitelist: Looking for these IAB categories in the bid request. Multi select. Choose none to accept any IAB Blacklist: Exluding these IAB categories. Multi select. Choose none to not exclude them. Select the purple Save to save the Target. After putting USA in the country field and pressing Save we see this result: Returning back to Campaigns, and editing \"Test-campaign-1\" we can select My New Target for it. Now, all we need to make this a viable campaign is to attach a creative. Creatives \u00b6 A creative is the actual thing the user will see on their device. This is your actual advertisement. Choosing the Creatives icon on the blue panel brings up the following screen: Banner \u00b6 A Banner creative is a simple graphical image, such as a gif, png, or jpg picture. The banners are accessed via the New button above the Banners field . Example after pressing new: And scrolling down to the end: The fields and their valus are described below: SQL ID: Not editable Name: Name of this creative REQUIRED ECPM/Price: Price per 1000 REQUIRED App/Site/Both: Whether this is aonly for apps, websites or both. REQUIRED Match Size: Any/Specified/W-H Ranges/W-H List: Choose one to set your sizes you are looking for REQUIRED Deals: No Deal/Private Only/Private Preferred: Which deal strategy if any. REQUIRED Content-Type: The type of the banner REQUIRED The URL of the image REQURED . Click Through URL: Where to go on the Click REQUIRED HTML Template: Actual HTML code of the tag. REQUIRED Campaign: Shows the attached campaign. Creative's Categories: Identifies the categories that match this ad. REQUIRED Creatices Atrributes: Multi selection of the attributes of the ad. REQUIRED Specialty Exchange Attributes. Press the button to input the special key fields required by the respective exchanges. REQUIRED FOR THOSE EXCANGES Ignore if these exchanges don't apply. Start date creative is active. REQURED End Date creative is active. REQUIRED Rules: Select any rules you want attched to this creative. Total Budget: Total amount of US dollars to allocate to this campaign. Setting 0 means no budget. Daily Budget: Amount campaign is to spend per day. Setting 0 means no daily constraint. Hourly Budget: Amount campaugn is to spend per hour. Setting 0 means no set hourly constraint. Select save to save the creative to the database. Your screen w ll show the new creative: Now with the creative, we can attach it to the campaign. Going back to Campaigns panel and editing Test-campaign-1 we can now select Test Banner in the Creatives section, and also select runnable for Bidder Status . Pressing Save will bring up the Campaigns status window, the status line should turn green because the bidders have now been loaded with a viable campaign. Video \u00b6 A video creative is a Video that plays in a VAST player. The video creative is accessed via the the \"Videos\" panel. An example is shown below: Audio \u00b6 Native \u00b6 Rules \u00b6 Sets \u00b6 Simulator \u00b6 User Profile \u00b6","title":"Campaign User Guide"},{"location":"home/userguides/#overview","text":"The Campaign Manager, the second layer of the DSP. Layer 1 is the Bidder, Layer 2 is the Campaign Manager and Layer 3 is the Data Management Platform. Features at a glance: REACT Web based. Supports Banner, Video, Audio and Native Ads. Provides Day Parting service. Employs Open, Hourly, Daily and Total budgeting. Easy to use targetting. Build your own specialized rules if standard targetting is not sufficient. Docker Microservices Architecture. Built with REACT. Multi-tenant. A campaign is the offering of one or more creatives over a certain time period, and with specific targets. We use the term target to mean constraining rules that filter out those id requests that match your target market. When we say reative we mean a banner ad, a video or a native ad. Additionally, you can use the Campaign Manager to review the the running status of your campaign, and to start and stop campaigns. The process to create a new campaign is: Select the Campaigns icon and define the basic campaign parameters Save the campaign. Select the Banners or Video icon and define the creative parameters. Select the campaign you created in step 1 to assign the creative to your campaign. Save the creative. Select the Target icon and define the target parameters. Save the target. Return to the Campaign edit page, and select the Target that you created. Saving the Campaign will load it in the bidder.","title":"Overview"},{"location":"home/userguides/#accessing","text":"To access the RTB4FREE Campaign Manager point your browser to http://localhost:8100/campaigns. The following screen should appear: On initial build, this user ben.faul@rtb4free.com is already configured. Use the following values to login the first time. Org: This should be rtb4free. This is the company id of the user, the database support multi-tenancy so multiple orgs can be supported. User: ben.faul@rtb4free.com - This is the user already configured. Password: zulu - This is the initial password. Server: localhost:7379. Point this to any bidder at this port id. This is the API port.","title":"Accessing"},{"location":"home/userguides/#dashboard","text":"On initially logging in , you will be at the dashboard, like this: The Instances show you how many bidders are running in this cluster. In this case only 1. The Running Campaigns shows how many campaigns are loaded into the cluster. In this case there are none.","title":"Dashboard"},{"location":"home/userguides/#bidding","text":"To send bids to the SSPs you need viable campaigns. A campaign is the offering of one or more creatives over a certain time period, and with specific targets. We use the term target to mean common constraints that filter out those bid requests to match your target market. When we say creative we mean a banner ad, a video or a native ad.","title":"Bidding"},{"location":"home/userguides/#campaigns","text":"Pressing the Campaign icon on the blue dashboard will bring up the following screen that shows existing campaigns, in this case there are no campaigns yet: Pressing the red New button will allow us to create a new campaign: And scrolling down a little: The following fields are set forth with their meanings: SQL idhis is a db id, it is nt editable. Name This is the name of the campaign. It must be a unique name. REQUIRED . Ad Domain: This is the advertisers root domain. REQUIRED Fraud Suppression: This indicates if built in fraud suppression is used, but no effect if not enabled at the system level. Bidder status: Whether this is an actve campaign. Set to online to bid (but will only go online if it is a viable campaign). Creatives: This list will show what creatives are assigned to this campaign. Not viable without creatives Start: The start date and time for campaign activation REQUIRED Stop: The stop time of campaign activation. REQUIRED Spend rate: Regardless of budget, maximum spend rate per minute of a campaign. Setting nothing or 0 means no limit. Bidding Region: Choose a region where this campaign is to be bid in, REQURED Exchanges: Choose none or multiple exchanges (if they are connected). Selecting none will enable all. Target: Choose a target to attach to the campaign Not viable without a target Rules: Choose 0 or more defined rules to attach to the campaign. A rule is a specialized constraint. Total Budget: Total amount of US dollars to allocate to this campaign. Setting 0 means no budget. Daily Budget: Amount campaign is to spend per day. Setting 0 means no daily constraint. Hourly Budget: Amount campaugn is to spend per hour. Setting 0 means no set hourly constraint. Frequency Specification: Defines the RTB spec JSON name to key on. For example device.ip Dayparting: Sets a calendar of when to spend by days of the week and by time. Push the purple SAVE button to save the campaign. The following is a sample campaign. Creating a campaign named \"Test-Campaign-1\" with no targets or creatives creates the campaign byt it will be offline as indicated by the grey stripe: Pressing the blue Report button will tell you why the campaign is offline. In this case \"No attached creatives. Marked offline\" is displayed in the popup after Report is pressed. Fields with \"*\" indicate a required field. Most of the fields are self-explanatory, with the exception of Region, and Rules. A Region is a bidding region. It is the name given to a group of bidders controlled by Crosstalk. Only one Region can be specified. If you are set up to bid in the US, then use US as the region. If in Asia, use AP, and if in Europe use EU. Rules are specific constraints applied to the campaign gathered from the bid requests and are discussed further in the documentation. Once you have a campaign created, you can add Banner and Video, Audio and Native Ad creatives.","title":"Campaigns"},{"location":"home/userguides/#targets","text":"The next step is to create a target. Pressing the Targets icon in the blue field causes the following to appear: Pressing the red New button will cause the new target window to appear: Here is a description of the fields and what they mean: SQL ID: The database key id, not editable. Ad id: This is the target's name REQUIRED . Domain Values: CCC. Use Big Data Set for Domain Values. Type: Select WHITELIST to make sure the domain values are present, otherwise BLACKLIST to exclude from bidding the value is present. Country: Put the country names in by 3 character ISO names. With nothing specified all countries are valid. Geo Latitiude: Put the GPS latitude here to create a fence. None means no fence. Geo Longitude: Put the GPS longitude here to create a fence. None meand no fence. Geo Range: Inclusive circle in Km around lat/lon for the fence. Note, for multiple fences use a rule. Carrier: Name of the carrier to key on. Case sensitive Comma separated list. Operating System: Name of OS. Case Sensitive Comma separated list. Make: A single make is allowed. Model: Models supported, one per line. Device types: Choose one or more. Choosing none selects all. IAB Whitelist: Looking for these IAB categories in the bid request. Multi select. Choose none to accept any IAB Blacklist: Exluding these IAB categories. Multi select. Choose none to not exclude them. Select the purple Save to save the Target. After putting USA in the country field and pressing Save we see this result: Returning back to Campaigns, and editing \"Test-campaign-1\" we can select My New Target for it. Now, all we need to make this a viable campaign is to attach a creative.","title":"Targets"},{"location":"home/userguides/#creatives","text":"A creative is the actual thing the user will see on their device. This is your actual advertisement. Choosing the Creatives icon on the blue panel brings up the following screen:","title":"Creatives"},{"location":"home/userguides/#banner","text":"A Banner creative is a simple graphical image, such as a gif, png, or jpg picture. The banners are accessed via the New button above the Banners field . Example after pressing new: And scrolling down to the end: The fields and their valus are described below: SQL ID: Not editable Name: Name of this creative REQUIRED ECPM/Price: Price per 1000 REQUIRED App/Site/Both: Whether this is aonly for apps, websites or both. REQUIRED Match Size: Any/Specified/W-H Ranges/W-H List: Choose one to set your sizes you are looking for REQUIRED Deals: No Deal/Private Only/Private Preferred: Which deal strategy if any. REQUIRED Content-Type: The type of the banner REQUIRED The URL of the image REQURED . Click Through URL: Where to go on the Click REQUIRED HTML Template: Actual HTML code of the tag. REQUIRED Campaign: Shows the attached campaign. Creative's Categories: Identifies the categories that match this ad. REQUIRED Creatices Atrributes: Multi selection of the attributes of the ad. REQUIRED Specialty Exchange Attributes. Press the button to input the special key fields required by the respective exchanges. REQUIRED FOR THOSE EXCANGES Ignore if these exchanges don't apply. Start date creative is active. REQURED End Date creative is active. REQUIRED Rules: Select any rules you want attched to this creative. Total Budget: Total amount of US dollars to allocate to this campaign. Setting 0 means no budget. Daily Budget: Amount campaign is to spend per day. Setting 0 means no daily constraint. Hourly Budget: Amount campaugn is to spend per hour. Setting 0 means no set hourly constraint. Select save to save the creative to the database. Your screen w ll show the new creative: Now with the creative, we can attach it to the campaign. Going back to Campaigns panel and editing Test-campaign-1 we can now select Test Banner in the Creatives section, and also select runnable for Bidder Status . Pressing Save will bring up the Campaigns status window, the status line should turn green because the bidders have now been loaded with a viable campaign.","title":"Banner"},{"location":"home/userguides/#video","text":"A video creative is a Video that plays in a VAST player. The video creative is accessed via the the \"Videos\" panel. An example is shown below:","title":"Video"},{"location":"home/userguides/#audio","text":"","title":"Audio"},{"location":"home/userguides/#native","text":"","title":"Native"},{"location":"home/userguides/#rules","text":"","title":"Rules"},{"location":"home/userguides/#sets","text":"","title":"Sets"},{"location":"home/userguides/#simulator","text":"","title":"Simulator"},{"location":"home/userguides/#user-profile","text":"","title":"User Profile"}]}